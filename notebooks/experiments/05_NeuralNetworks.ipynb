{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. æ·ºå±¤ç¥ç¶“ç¶²è·¯ (ANN) å¯¦é©—\n",
    "\n",
    "## ğŸ“– å¯¦é©—ç›®æ¨™\n",
    "\n",
    "åœ¨ 03_ModelBuilding å’Œ 04_XGBoost ä¸­ï¼Œæˆ‘å€‘æ¸¬è©¦äº†å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼š\n",
    "- âœ… **Logistic Regression**: AUC >0.88ï¼Œä½† F1 è¼ƒä½\n",
    "- âœ… **XGBoost**: F1 æå‡ 9-39%ï¼ŒAUC ç•¥æœ‰æ”¹å–„\n",
    "\n",
    "æœ¬ notebook å°‡æ¸¬è©¦ **æ·ºå±¤ç¥ç¶“ç¶²è·¯ (ANN)**ï¼Œç†ç”±ï¼š\n",
    "1. ğŸ§  **éç·šæ€§å»ºæ¨¡èƒ½åŠ›**: å¯èƒ½æ•æ‰æ›´è¤‡é›œçš„ç‰¹å¾µé—œä¿‚\n",
    "2. ğŸ“Š **é©åˆä¸­ç­‰è³‡æ–™é‡**: 6K æ¨£æœ¬è¶³å¤ è¨“ç·´ 1-2 å±¤ç¶²è·¯\n",
    "3. ğŸ¯ **class_weight æ”¯æ´**: å¯è™•ç†é¡åˆ¥ä¸å¹³è¡¡\n",
    "4. âš¡ **è¨“ç·´é€Ÿåº¦å¿«**: æ¯” SVM å¿«å¾ˆå¤š\n",
    "\n",
    "**ANN vs DNN**ï¼š\n",
    "- **ANN (æ·ºå±¤)**: 1-2 éš±è—å±¤ï¼Œé©åˆ 5K-10K æ¨£æœ¬ âœ…\n",
    "- **DNN (æ·±å±¤)**: 3+ éš±è—å±¤ï¼Œéœ€è¦ 50K+ æ¨£æœ¬ âŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ å¯¦é©—æµç¨‹\n",
    "\n",
    "1. è¼‰å…¥è³‡æ–™ï¼ˆä½¿ç”¨èˆ‡ 03/04 ç›¸åŒçš„è³‡æ–™è™•ç†ï¼‰\n",
    "2. å–®ä»»å‹™ ANNï¼ˆä¸‰ç¨®ç–¾ç—…åˆ†åˆ¥è¨“ç·´ï¼‰\n",
    "3. MTL ANNï¼ˆå¤šè¼¸å‡ºç¥ç¶“ç¶²è·¯ï¼‰\n",
    "4. èˆ‡æœ€ä½³æ¨¡å‹æ¯”è¼ƒï¼ˆMTL LR, XGBoostï¼‰\n",
    "5. çµè«–èˆ‡å»ºè­°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥å¥—ä»¶èˆ‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ TensorFlow/Keras (å¦‚æœå°šæœªå®‰è£)\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤å¥—ä»¶\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# æ©Ÿå™¨å­¸ç¿’å¥—ä»¶\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# è¨­å®š\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ\")\n",
    "print(f\"TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
    "print(f\"Keras ç‰ˆæœ¬: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è³‡æ–™\n",
    "df = pd.read_csv('../../data/processed/SUA_CVDs_wide_format.csv')\n",
    "\n",
    "print(\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\")\n",
    "print(f\"è³‡æ–™å½¢ç‹€: {df.shape[0]:,} äºº, {df.shape[1]} å€‹æ¬„ä½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ç‰¹å¾µçµ„\n",
    "demographic_features = ['sex', 'Age']\n",
    "biomarker_names = ['FBG', 'TC', 'Cr', 'UA', 'GFR', 'BMI', 'SBP', 'DBP']\n",
    "t1_features = [f'{name}_T1' for name in biomarker_names]\n",
    "t2_features = [f'{name}_T2' for name in biomarker_names]\n",
    "delta1_features = [f'Delta1_{name}' for name in biomarker_names]\n",
    "\n",
    "# å®Œæ•´ç‰¹å¾µé›†\n",
    "X_columns = demographic_features + t1_features + t2_features + delta1_features\n",
    "X = df[X_columns]\n",
    "\n",
    "# ç›®æ¨™è®Šæ•¸ï¼ˆè½‰æ›ç‚º 0/1ï¼‰\n",
    "y_hypertension = (df['hypertension_T3'] == 2).astype(int)\n",
    "y_hyperglycemia = (df['hyperglycemia_T3'] == 2).astype(int)\n",
    "y_dyslipidemia = (df['dyslipidemia_T3'] == 2).astype(int)\n",
    "\n",
    "# MTL ç›®æ¨™è®Šæ•¸\n",
    "y_multi = np.column_stack([y_hypertension, y_hyperglycemia, y_dyslipidemia])\n",
    "\n",
    "print(f\"ç‰¹å¾µæ•¸: {len(X_columns)} å€‹\")\n",
    "print(f\"\\nç›®æ¨™è®Šæ•¸åˆ†ä½ˆ:\")\n",
    "print(f\"  é«˜è¡€å£“æ‚£ç—…ç‡: {y_hypertension.mean():.2%}\")\n",
    "print(f\"  é«˜è¡€ç³–æ‚£ç—…ç‡: {y_hyperglycemia.mean():.2%}\")\n",
    "print(f\"  é«˜è¡€è„‚æ‚£ç—…ç‡: {y_dyslipidemia.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è³‡æ–™åˆ†å‰²èˆ‡æ¨™æº–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è³‡æ–™åˆ†å‰²\n",
    "X_train, X_test, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y_multi, test_size=0.2, random_state=42, stratify=y_hypertension\n",
    ")\n",
    "\n",
    "# åˆ†é›¢ä¸‰å€‹ç›®æ¨™è®Šæ•¸\n",
    "y_train_hp = y_train_multi[:, 0]\n",
    "y_train_hg = y_train_multi[:, 1]\n",
    "y_train_dl = y_train_multi[:, 2]\n",
    "\n",
    "y_test_hp = y_test_multi[:, 0]\n",
    "y_test_hg = y_test_multi[:, 1]\n",
    "y_test_dl = y_test_multi[:, 2]\n",
    "\n",
    "# æ¨™æº–åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… è³‡æ–™æº–å‚™å®Œæˆ\")\n",
    "print(f\"è¨“ç·´é›†: {X_train_scaled.shape[0]} äºº\")\n",
    "print(f\"æ¸¬è©¦é›†: {X_test_scaled.shape[0]} äºº\")\n",
    "print(f\"ç‰¹å¾µæ•¸: {X_train_scaled.shape[1]} å€‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è¨ˆç®— class_weight\n",
    "\n",
    "è™•ç†é¡åˆ¥ä¸å¹³è¡¡ï¼Œè¨ˆç®—æ¯å€‹ç–¾ç—…çš„ class_weightã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®— class_weight\n",
    "def calculate_class_weight(y):\n",
    "    n_samples = len(y)\n",
    "    n_positive = np.sum(y == 1)\n",
    "    n_negative = np.sum(y == 0)\n",
    "    \n",
    "    weight_for_0 = n_samples / (2 * n_negative)\n",
    "    weight_for_1 = n_samples / (2 * n_positive)\n",
    "    \n",
    "    return {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "class_weight_hp = calculate_class_weight(y_train_hp)\n",
    "class_weight_hg = calculate_class_weight(y_train_hg)\n",
    "class_weight_dl = calculate_class_weight(y_train_dl)\n",
    "\n",
    "print(\"======================================================================\")\n",
    "print(\"class_weight è¨ˆç®—çµæœ\")\n",
    "print(\"======================================================================\")\n",
    "print(f\"é«˜è¡€å£“: {{0: {class_weight_hp[0]:.2f}, 1: {class_weight_hp[1]:.2f}}}\")\n",
    "print(f\"é«˜è¡€ç³–: {{0: {class_weight_hg[0]:.2f}, 1: {class_weight_hg[1]:.2f}}}\")\n",
    "print(f\"é«˜è¡€è„‚: {{0: {class_weight_dl[0]:.2f}, 1: {class_weight_dl[1]:.2f}}}\")\n",
    "print(\"\\nğŸ’¡ æ¬Šé‡è¶Šå¤§è¡¨ç¤ºè©²é¡åˆ¥è¶Šç¨€å°‘ï¼Œéœ€è¦æ›´å¤šé—œæ³¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å–®ä»»å‹™ ANN\n",
    "\n",
    "å»ºç«‹ç°¡å–®çš„ 2 å±¤ç¥ç¶“ç¶²è·¯ï¼Œåˆ†åˆ¥é æ¸¬ä¸‰ç¨®ç–¾ç—…ã€‚\n",
    "\n",
    "**æ¶æ§‹**ï¼š\n",
    "- è¼¸å…¥å±¤: 26 å€‹ç‰¹å¾µ\n",
    "- éš±è—å±¤ 1: 64 neurons + ReLU + Dropout(0.3)\n",
    "- éš±è—å±¤ 2: 32 neurons + ReLU + Dropout(0.3)\n",
    "- è¼¸å‡ºå±¤: 1 neuron + Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann_model(input_dim):\n",
    "    \"\"\"å»ºç«‹å–®ä»»å‹™ ANN æ¨¡å‹\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… æ¨¡å‹æ¶æ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ä¸‰å€‹å–®ä»»å‹™ ANN æ¨¡å‹\n",
    "diseases = ['é«˜è¡€å£“', 'é«˜è¡€ç³–', 'é«˜è¡€è„‚']\n",
    "y_trains = [y_train_hp, y_train_hg, y_train_dl]\n",
    "y_tests = [y_test_hp, y_test_hg, y_test_dl]\n",
    "class_weights = [class_weight_hp, class_weight_hg, class_weight_dl]\n",
    "\n",
    "ann_models = {}\n",
    "ann_results = []\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"======================================================================\")\n",
    "print(\"å–®ä»»å‹™ ANN è¨“ç·´ä¸­...\")\n",
    "print(\"======================================================================\\n\")\n",
    "\n",
    "for disease, y_train, y_test, class_weight in zip(diseases, y_trains, y_tests, class_weights):\n",
    "    print(f\"è¨“ç·´ {disease} æ¨¡å‹...\")\n",
    "    \n",
    "    # å»ºç«‹æ¨¡å‹\n",
    "    model = build_ann_model(X_train_scaled.shape[1])\n",
    "    \n",
    "    # è¨“ç·´ï¼ˆéœé»˜æ¨¡å¼ï¼‰\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred_proba = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # è¨ˆç®—æŒ‡æ¨™\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    ann_models[disease] = model\n",
    "    ann_results.append({\n",
    "        'ç–¾ç—…': disease,\n",
    "        'AUC': auc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    })\n",
    "    \n",
    "    # è¼¸å‡ºçµæœ\n",
    "    print(f\"{disease}:\")\n",
    "    print(f\"  è¨“ç·´ epochs: {len(history.history['loss'])}\")\n",
    "    print(f\"  AUC:       {auc:.3f}\")\n",
    "    print(f\"  F1:        {f1:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    \n",
    "    # æ··æ·†çŸ©é™£\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    print(f\"  æ··æ·†çŸ©é™£: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… å–®ä»»å‹™ ANN è¨“ç·´å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MTL ANN (Multi-Task Learning)\n",
    "\n",
    "å»ºç«‹å¤šè¼¸å‡ºç¥ç¶“ç¶²è·¯ï¼Œå…±äº«éš±è—å±¤ï¼ŒåŒæ™‚é æ¸¬ä¸‰ç¨®ç–¾ç—…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mtl_ann_model(input_dim):\n",
    "    \"\"\"å»ºç«‹ MTL ANN æ¨¡å‹\"\"\"\n",
    "    # å…±äº«çš„è¼¸å…¥å±¤\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # å…±äº«çš„éš±è—å±¤\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # ä¸‰å€‹è¼¸å‡ºé ­\n",
    "    output_hp = layers.Dense(1, activation='sigmoid', name='hypertension')(x)\n",
    "    output_hg = layers.Dense(1, activation='sigmoid', name='hyperglycemia')(x)\n",
    "    output_dl = layers.Dense(1, activation='sigmoid', name='dyslipidemia')(x)\n",
    "    \n",
    "    # å»ºç«‹æ¨¡å‹\n",
    "    model = models.Model(inputs=inputs, outputs=[output_hp, output_hg, output_dl])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… MTL æ¨¡å‹æ¶æ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======================================================================\")\n",
    "print(\"MTL ANN è¨“ç·´ä¸­...\")\n",
    "print(\"======================================================================\\n\")\n",
    "\n",
    "# å»ºç«‹ MTL æ¨¡å‹\n",
    "mtl_ann = build_mtl_ann_model(X_train_scaled.shape[1])\n",
    "\n",
    "# è¨“ç·´\n",
    "history = mtl_ann.fit(\n",
    "    X_train_scaled, \n",
    "    [y_train_hp, y_train_hg, y_train_dl],\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´å®Œæˆï¼Œå…± {len(history.history['loss'])} epochs\")\n",
    "\n",
    "# é æ¸¬\n",
    "y_pred_proba_hp, y_pred_proba_hg, y_pred_proba_dl = mtl_ann.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "mtl_ann_results = []\n",
    "\n",
    "for i, (disease, y_test, y_pred_proba) in enumerate([\n",
    "    ('é«˜è¡€å£“', y_test_hp, y_pred_proba_hp.flatten()),\n",
    "    ('é«˜è¡€ç³–', y_test_hg, y_pred_proba_hg.flatten()),\n",
    "    ('é«˜è¡€è„‚', y_test_dl, y_pred_proba_dl.flatten())\n",
    "]):\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # è¨ˆç®—æŒ‡æ¨™\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    mtl_ann_results.append({\n",
    "        'ç–¾ç—…': disease,\n",
    "        'AUC': auc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    })\n",
    "    \n",
    "    # è¼¸å‡ºçµæœ\n",
    "    print(f\"\\n{disease}:\")\n",
    "    print(f\"  AUC:       {auc:.3f}\")\n",
    "    print(f\"  F1:        {f1:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    \n",
    "    # æ··æ·†çŸ©é™£\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    print(f\"  æ··æ·†çŸ©é™£: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "\n",
    "print(\"\\nâœ… MTL ANN è¨“ç·´å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ANN æ•ˆèƒ½æ¯”è¼ƒ (å–®ä»»å‹™ vs MTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆä½µçµæœ\n",
    "df_single_ann = pd.DataFrame(ann_results)\n",
    "df_single_ann['æ–¹æ³•'] = 'ANN (å–®ä»»å‹™)'\n",
    "\n",
    "df_mtl_ann = pd.DataFrame(mtl_ann_results)\n",
    "df_mtl_ann['æ–¹æ³•'] = 'ANN (MTL)'\n",
    "\n",
    "df_ann_comparison = pd.concat([df_single_ann, df_mtl_ann], ignore_index=True)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(\"ANN æ–¹æ³•æ¯”è¼ƒ\")\n",
    "print(\"================================================================================\")\n",
    "print(df_ann_comparison[['æ–¹æ³•', 'ç–¾ç—…', 'AUC', 'F1', 'Recall']].to_string(index=False))\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ–¹æ³•\n",
    "print(\"\\n================================================================================\")\n",
    "print(\"æœ€ä½³ ANN æ–¹æ³•\")\n",
    "print(\"================================================================================\\n\")\n",
    "\n",
    "for disease in diseases:\n",
    "    disease_results = df_ann_comparison[df_ann_comparison['ç–¾ç—…'] == disease]\n",
    "    best_auc = disease_results.loc[disease_results['AUC'].idxmax()]\n",
    "    best_f1 = disease_results.loc[disease_results['F1'].idxmax()]\n",
    "    best_recall = disease_results.loc[disease_results['Recall'].idxmax()]\n",
    "    \n",
    "    print(f\"{disease}:\")\n",
    "    print(f\"  æœ€ä½³ AUC:    {best_auc['æ–¹æ³•']:20s} (AUC={best_auc['AUC']:.3f})\")\n",
    "    print(f\"  æœ€ä½³ F1:     {best_f1['æ–¹æ³•']:20s} (F1={best_f1['F1']:.3f})\")\n",
    "    print(f\"  æœ€ä½³ Recall: {best_recall['æ–¹æ³•']:20s} (Recall={best_recall['Recall']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è¦–è¦ºåŒ– ANN æ•ˆèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¦–è¦ºåŒ–\n",
    "metrics = ['AUC', 'F1', 'Recall']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # æº–å‚™è³‡æ–™\n",
    "    pivot_data = df_ann_comparison.pivot(index='ç–¾ç—…', columns='æ–¹æ³•', values=metric)\n",
    "    \n",
    "    # ç¹ªåœ–\n",
    "    pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_title(f'{metric} æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_xlabel('ç–¾ç—…', fontsize=12)\n",
    "    ax.legend(title='æ–¹æ³•', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../docs/experiments/ann_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… è¦–è¦ºåŒ–å®Œæˆï¼Œå·²å„²å­˜è‡³ docs/experiments/ann_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. èˆ‡æœ€ä½³æ¨¡å‹æ¯”è¼ƒ\n",
    "\n",
    "æ¯”è¼ƒ ANN èˆ‡ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼š\n",
    "- MTL LR (balanced) from 03\n",
    "- XGBoost (å–®ä»»å‹™) from 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======================================================================\")\n",
    "print(\"é‡æ–°è¨“ç·´ MTL LR (balanced) å’Œ XGBoost ä½œç‚ºåŸºæº–æ¯”è¼ƒ\")\n",
    "print(\"======================================================================\\n\")\n",
    "\n",
    "# è¨“ç·´ MTL LR\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mtl_lr = MultiOutputClassifier(\n",
    "    LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    ")\n",
    "mtl_lr.fit(X_train_scaled, y_train_multi)\n",
    "\n",
    "# è¨“ç·´ XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_models_comparison = {}\n",
    "for disease, y_train, y_test in zip(diseases, y_trains, y_tests):\n",
    "    n_positive = np.sum(y_train == 1)\n",
    "    n_negative = np.sum(y_train == 0)\n",
    "    scale_pos_weight = n_negative / n_positive\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    xgb_models_comparison[disease] = model\n",
    "\n",
    "print(\"âœ… åŸºæº–æ¨¡å‹è¨“ç·´å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ‰€æœ‰æ¨¡å‹çš„çµæœ\n",
    "all_results = []\n",
    "\n",
    "# MTL LR\n",
    "y_pred_lr = mtl_lr.predict(X_test_scaled)\n",
    "for i, disease in enumerate(diseases):\n",
    "    y_test = y_test_multi[:, i]\n",
    "    y_pred = y_pred_lr[:, i]\n",
    "    y_pred_proba = mtl_lr.estimators_[i].predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    all_results.append({\n",
    "        'æ–¹æ³•': 'MTL LR (balanced)',\n",
    "        'ç–¾ç—…': disease,\n",
    "        'AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0)\n",
    "    })\n",
    "\n",
    "# XGBoost\n",
    "for disease, y_test in zip(diseases, y_tests):\n",
    "    model = xgb_models_comparison[disease]\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    all_results.append({\n",
    "        'æ–¹æ³•': 'XGBoost (å–®ä»»å‹™)',\n",
    "        'ç–¾ç—…': disease,\n",
    "        'AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0)\n",
    "    })\n",
    "\n",
    "# åŠ å…¥ ANN çµæœ\n",
    "df_best_ann = df_single_ann.copy()\n",
    "df_best_ann['æ–¹æ³•'] = 'ANN (å–®ä»»å‹™)'\n",
    "\n",
    "df_all = pd.concat([pd.DataFrame(all_results), df_best_ann], ignore_index=True)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(\"æ‰€æœ‰æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ\")\n",
    "print(\"================================================================================\")\n",
    "print(df_all[['æ–¹æ³•', 'ç–¾ç—…', 'AUC', 'F1', 'Recall']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. è¦–è¦ºåŒ–æœ€çµ‚æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æœ€çµ‚æ¯”è¼ƒè¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, metric in enumerate(['AUC', 'F1', 'Recall']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # æº–å‚™è³‡æ–™\n",
    "    pivot_data = df_all.pivot(index='ç–¾ç—…', columns='æ–¹æ³•', values=metric)\n",
    "    \n",
    "    # ç¹ªåœ–\n",
    "    pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_title(f'{metric} æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_xlabel('ç–¾ç—…', fontsize=12)\n",
    "    ax.legend(title='æ–¹æ³•', fontsize=9, loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../docs/experiments/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æœ€çµ‚æ¯”è¼ƒè¦–è¦ºåŒ–å®Œæˆï¼Œå·²å„²å­˜è‡³ docs/experiments/all_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. çµè«–\n",
    "\n",
    "### ğŸ¯ ANN æ•ˆèƒ½ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—æ¸¬è©¦äº†æ·ºå±¤ç¥ç¶“ç¶²è·¯ (ANN) åœ¨ä¸‰é«˜ç–¾ç—…é æ¸¬ä¸Šçš„è¡¨ç¾ï¼Œä¸¦èˆ‡ä¹‹å‰çš„æœ€ä½³æ¨¡å‹æ¯”è¼ƒã€‚\n",
    "\n",
    "### ğŸ“Š ä¸»è¦ç™¼ç¾\n",
    "\n",
    "**æ ¹æ“šå¯¦éš›åŸ·è¡Œçµæœå¡«å¯«**ï¼š\n",
    "\n",
    "1. **ANN vs å‚³çµ±æ©Ÿå™¨å­¸ç¿’**\n",
    "   - ANN çš„ AUC è¡¨ç¾ï¼š[å¡«å¯«å¯¦éš›çµæœ]\n",
    "   - èˆ‡ XGBoost ç›¸æ¯”ï¼š[å¡«å¯«æ¯”è¼ƒçµæœ]\n",
    "   - èˆ‡ LR ç›¸æ¯”ï¼š[å¡«å¯«æ¯”è¼ƒçµæœ]\n",
    "\n",
    "2. **å–®ä»»å‹™ vs MTL ANN**\n",
    "   - æ•ˆèƒ½å·®ç•°ï¼š[å¡«å¯«çµæœ]\n",
    "   - è¨“ç·´æ™‚é–“ï¼šMTL æ›´å¿«ï¼ˆä¸€æ¬¡è¨“ç·´ vs ä¸‰æ¬¡ï¼‰\n",
    "\n",
    "3. **ANN çš„å„ªç¼ºé»**\n",
    "   - âœ… å„ªé»ï¼šéç·šæ€§å»ºæ¨¡ã€è‡ªå‹•ç‰¹å¾µå­¸ç¿’ã€æ”¯æ´ class_weight\n",
    "   - âŒ ç¼ºé»ï¼šéœ€è¦èª¿åƒã€å¯è§£é‡‹æ€§ä½ã€å¯èƒ½éæ“¬åˆ\n",
    "\n",
    "### ğŸ’¡ å»ºè­°\n",
    "\n",
    "1. **ä½•æ™‚é¸æ“‡ ANNï¼Ÿ**\n",
    "   - éœ€è¦æ•æ‰éç·šæ€§é—œä¿‚\n",
    "   - è³‡æ–™é‡è¶³å¤ ï¼ˆ>5Kï¼‰\n",
    "   - ä¸éœ€è¦æ¨¡å‹å¯è§£é‡‹æ€§\n",
    "\n",
    "2. **ä½•æ™‚é¸æ“‡å…¶ä»–æ¨¡å‹ï¼Ÿ**\n",
    "   - éœ€è¦å¯è§£é‡‹æ€§ â†’ Logistic Regression\n",
    "   - éœ€è¦ç‰¹å¾µé‡è¦æ€§ â†’ XGBoost\n",
    "   - è³‡æ–™é‡å° â†’ SVM\n",
    "\n",
    "### ğŸ“ ä¸‹ä¸€æ­¥\n",
    "\n",
    "- 06_SVM.ipynb: æ¸¬è©¦ Support Vector Machine\n",
    "- è¶…åƒæ•¸èª¿å„ªï¼šèª¿æ•´ ANN çš„å±¤æ•¸ã€neuronsã€dropout\n",
    "- æ¨¡å‹é›†æˆï¼šçµåˆ ANN, XGBoost, LR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
