{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ©Ÿå™¨å­¸ç¿’æ¨¡å‹å»ºç«‹ - ä¸‰é«˜é æ¸¬\n",
    "\n",
    "**ç›®æ¨™**: ä½¿ç”¨è™•ç†å¥½çš„è³‡æ–™å»ºç«‹é æ¸¬æ¨¡å‹\n",
    "\n",
    "**ç­–ç•¥**:\n",
    "- ä½¿ç”¨ T1 + T2 çš„è³‡æ–™è¨“ç·´æ¨¡å‹\n",
    "- é æ¸¬ T3 æ™‚çš„ä¸‰é«˜ç‹€æ…‹\n",
    "- å¾ç°¡å–®çš„ Logistic Regression é–‹å§‹\n",
    "\n",
    "**æ—¥æœŸ**: 2025-11-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥å¥—ä»¶èˆ‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# æ©Ÿå™¨å­¸ç¿’å¥—ä»¶\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šä¸­æ–‡å­—å‹\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Microsoft YaHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"å¥—ä»¶è¼‰å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è™•ç†å¥½çš„è³‡æ–™\n",
    "data_path = Path('../../data/processed/SUA_CVDs_wide_format.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\")\n",
    "print(f\"è³‡æ–™å½¢ç‹€: {df.shape[0]:,} äºº, {df.shape[1]} å€‹æ¬„ä½\")\n",
    "print(f\"\\næ¬„ä½é è¦½:\")\n",
    "print(df.columns[:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸\n",
    "\n",
    "æˆ‘å€‘å°‡ä½¿ç”¨ **T1 å’Œ T2 çš„è³‡æ–™** ä¾†é æ¸¬ **T3 çš„ä¸‰é«˜ç‹€æ…‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ç‰¹å¾µæ¬„ä½ï¼ˆä½¿ç”¨ T1 å’Œ T2 çš„è³‡æ–™ï¼‰\n",
    "# åŸºç¤ç‰¹å¾µ\n",
    "t1_features = ['FBG_T1', 'TC_T1', 'Cr_T1', 'UA_T1', 'GFR_T1', 'BMI_T1', 'SBP_T1', 'DBP_T1']\n",
    "t2_features = ['FBG_T2', 'TC_T2', 'Cr_T2', 'UA_T2', 'GFR_T2', 'BMI_T2', 'SBP_T2', 'DBP_T2']\n",
    "\n",
    "# Î” è®ŠåŒ–ç‰¹å¾µï¼ˆT2 - T1ï¼‰\n",
    "delta1_features = ['Delta1_FBG', 'Delta1_TC', 'Delta1_Cr', 'Delta1_UA', \n",
    "                   'Delta1_GFR', 'Delta1_BMI', 'Delta1_SBP', 'Delta1_DBP']\n",
    "\n",
    "# äººå£çµ±è¨ˆç‰¹å¾µ\n",
    "demographic_features = ['sex', 'Age']\n",
    "\n",
    "# çµ„åˆæ‰€æœ‰ç‰¹å¾µ\n",
    "all_features = demographic_features + t1_features + t2_features + delta1_features\n",
    "\n",
    "print(f\"ç‰¹å¾µç¸½æ•¸: {len(all_features)} å€‹\")\n",
    "print(f\"  - äººå£çµ±è¨ˆ: {len(demographic_features)} å€‹\")\n",
    "print(f\"  - T1 ç‰¹å¾µ: {len(t1_features)} å€‹\")\n",
    "print(f\"  - T2 ç‰¹å¾µ: {len(t2_features)} å€‹\")\n",
    "print(f\"  - Î”1 ç‰¹å¾µ: {len(delta1_features)} å€‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ç›®æ¨™è®Šæ•¸ï¼ˆT3 æ™‚çš„ä¸‰é«˜ç‹€æ…‹ï¼‰\n",
    "target_columns = ['hypertension_T3', 'hyperglycemia_T3', 'dyslipidemia_T3']\n",
    "\n",
    "# æº–å‚™ Xï¼ˆç‰¹å¾µï¼‰å’Œ yï¼ˆç›®æ¨™ï¼‰\n",
    "X = df[all_features]\n",
    "y_hypertension = df['hypertension_T3']\n",
    "y_hyperglycemia = df['hyperglycemia_T3']\n",
    "y_dyslipidemia = df['dyslipidemia_T3']\n",
    "\n",
    "print(\"ç›®æ¨™è®Šæ•¸åˆ†ä½ˆ:\")\n",
    "print(f\"\\né«˜è¡€å£“ (hypertension_T3):\")\n",
    "print(y_hypertension.value_counts())\n",
    "print(f\"  æ‚£ç—…ç‡: {(y_hypertension == 2).mean():.2%}\")\n",
    "\n",
    "print(f\"\\né«˜è¡€ç³– (hyperglycemia_T3):\")\n",
    "print(y_hyperglycemia.value_counts())\n",
    "print(f\"  æ‚£ç—…ç‡: {(y_hyperglycemia == 2).mean():.2%}\")\n",
    "\n",
    "print(f\"\\né«˜è¡€è„‚ (dyslipidemia_T3):\")\n",
    "print(y_dyslipidemia.value_counts())\n",
    "print(f\"  æ‚£ç—…ç‡: {(y_dyslipidemia == 2).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è³‡æ–™åˆ†å‰²èˆ‡æ¨™æº–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡ç›®æ¨™è®Šæ•¸è½‰æ›ç‚ºäºŒå…ƒåˆ†é¡ï¼ˆ1=æ­£å¸¸, 2=æ‚£ç—… â†’ 0=æ­£å¸¸, 1=æ‚£ç—…ï¼‰\n",
    "y_hypertension_binary = (y_hypertension == 2).astype(int)\n",
    "y_hyperglycemia_binary = (y_hyperglycemia == 2).astype(int)\n",
    "y_dyslipidemia_binary = (y_dyslipidemia == 2).astype(int)\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼ˆ80% è¨“ç·´, 20% æ¸¬è©¦ï¼‰\n",
    "X_train, X_test, y_train_hp, y_test_hp = train_test_split(\n",
    "    X, y_hypertension_binary, test_size=0.2, random_state=42, stratify=y_hypertension_binary\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ç›¸åŒçš„åˆ†å‰²ç´¢å¼•ä¾†åˆ†å‰²å…¶ä»–ç›®æ¨™è®Šæ•¸\n",
    "_, _, y_train_hg, y_test_hg = train_test_split(\n",
    "    X, y_hyperglycemia_binary, test_size=0.2, random_state=42, stratify=y_hyperglycemia_binary\n",
    ")\n",
    "_, _, y_train_dl, y_test_dl = train_test_split(\n",
    "    X, y_dyslipidemia_binary, test_size=0.2, random_state=42, stratify=y_dyslipidemia_binary\n",
    ")\n",
    "\n",
    "print(\"è³‡æ–™åˆ†å‰²å®Œæˆ:\")\n",
    "print(f\"è¨“ç·´é›†: {X_train.shape[0]} äºº\")\n",
    "print(f\"æ¸¬è©¦é›†: {X_test.shape[0]} äºº\")\n",
    "print(f\"ç‰¹å¾µæ•¸: {X_train.shape[1]} å€‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨™æº–åŒ–ç‰¹å¾µ\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… ç‰¹å¾µæ¨™æº–åŒ–å®Œæˆ\")\n",
    "print(f\"è¨“ç·´é›†å½¢ç‹€: {X_train_scaled.shape}\")\n",
    "print(f\"æ¸¬è©¦é›†å½¢ç‹€: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å»ºç«‹ Baseline æ¨¡å‹ - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, model_name, disease_name):\n",
    "    \"\"\"è¨“ç·´æ¨¡å‹ä¸¦è©•ä¼°æ•ˆèƒ½\"\"\"\n",
    "    \n",
    "    # å»ºç«‹æ¨¡å‹\n",
    "    if model_name == 'Logistic Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif model_name == 'Random Forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # è¨ˆç®—æŒ‡æ¨™\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{disease_name} - {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"æº–ç¢ºç‡ (Accuracy):  {accuracy:.3f}\")\n",
    "    print(f\"ç²¾ç¢ºç‡ (Precision): {precision:.3f}\")\n",
    "    print(f\"å¬å›ç‡ (Recall):    {recall:.3f}\")\n",
    "    print(f\"F1 åˆ†æ•¸:            {f1:.3f}\")\n",
    "    print(f\"AUC:                {auc:.3f}\")\n",
    "    \n",
    "    # æ··æ·†çŸ©é™£\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "    print(f\"é æ¸¬ â†’    æ­£å¸¸  æ‚£ç—…\")\n",
    "    print(f\"å¯¦éš› â†“\")\n",
    "    print(f\"æ­£å¸¸      {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"æ‚£ç—…      {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "    \n",
    "    return model, {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´é«˜è¡€å£“é æ¸¬æ¨¡å‹\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"#\" + \" \"*20 + \"é«˜è¡€å£“é æ¸¬æ¨¡å‹\" + \" \"*24 + \"#\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "model_hp, metrics_hp = train_and_evaluate_model(\n",
    "    X_train_scaled, X_test_scaled, \n",
    "    y_train_hp, y_test_hp,\n",
    "    'Logistic Regression',\n",
    "    'é«˜è¡€å£“ (Hypertension)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´é«˜è¡€ç³–é æ¸¬æ¨¡å‹\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"#\" + \" \"*20 + \"é«˜è¡€ç³–é æ¸¬æ¨¡å‹\" + \" \"*24 + \"#\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "model_hg, metrics_hg = train_and_evaluate_model(\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    y_train_hg, y_test_hg,\n",
    "    'Logistic Regression',\n",
    "    'é«˜è¡€ç³– (Hyperglycemia)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´é«˜è¡€è„‚é æ¸¬æ¨¡å‹\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"#\" + \" \"*20 + \"é«˜è¡€è„‚é æ¸¬æ¨¡å‹\" + \" \"*24 + \"#\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "model_dl, metrics_dl = train_and_evaluate_model(\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    y_train_dl, y_test_dl,\n",
    "    'Logistic Regression',\n",
    "    'é«˜è¡€è„‚ (Dyslipidemia)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¯”è¼ƒä¸åŒç‰¹å¾µçµ„åˆçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ä¸åŒçš„ç‰¹å¾µçµ„åˆ\n",
    "feature_sets = {\n",
    "    'åªç”¨ T1 ç‰¹å¾µ': demographic_features + t1_features,\n",
    "    'T1 + Î”1 ç‰¹å¾µ': demographic_features + t1_features + delta1_features,\n",
    "    'å®Œæ•´ç‰¹å¾µ (T1+T2+Î”1)': all_features\n",
    "}\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"æ¯”è¼ƒä¸åŒç‰¹å¾µçµ„åˆ - é«˜è¡€å£“é æ¸¬\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, features in feature_sets.items():\n",
    "    # æº–å‚™è³‡æ–™\n",
    "    X_subset = df[features]\n",
    "    X_train_subset, X_test_subset, y_train, y_test = train_test_split(\n",
    "        X_subset, y_hypertension_binary, test_size=0.2, random_state=42, stratify=y_hypertension_binary\n",
    "    )\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_subset = scaler.fit_transform(X_train_subset)\n",
    "    X_test_subset = scaler.transform(X_test_subset)\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # é æ¸¬å’Œè©•ä¼°\n",
    "    y_pred = model.predict(X_test_subset)\n",
    "    y_pred_proba = model.predict_proba(X_test_subset)[:, 1]\n",
    "    \n",
    "    # è¨ˆç®—æŒ‡æ¨™\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'AUC': auc, 'F1': f1}\n",
    "    \n",
    "    print(f\"\\n{name} ({len(features)} å€‹ç‰¹å¾µ):\")\n",
    "    print(f\"  AUC: {auc:.3f}\")\n",
    "    print(f\"  F1:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è¦–è¦ºåŒ–æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´åˆä¸‰å€‹ç–¾ç—…çš„é æ¸¬çµæœ\n",
    "all_metrics = pd.DataFrame({\n",
    "    'é«˜è¡€å£“': metrics_hp,\n",
    "    'é«˜è¡€ç³–': metrics_hg,\n",
    "    'é«˜è¡€è„‚': metrics_dl\n",
    "}).T\n",
    "\n",
    "# ç¹ªè£½æ¯”è¼ƒåœ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# åœ–1: å„é …æŒ‡æ¨™æ¯”è¼ƒ\n",
    "all_metrics[['accuracy', 'precision', 'recall', 'f1']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('ä¸‰é«˜é æ¸¬æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('ç–¾ç—…é¡å‹')\n",
    "axes[0].set_ylabel('åˆ†æ•¸')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(['æº–ç¢ºç‡', 'ç²¾ç¢ºç‡', 'å¬å›ç‡', 'F1åˆ†æ•¸'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# åœ–2: AUC æ¯”è¼ƒ\n",
    "all_metrics['auc'].plot(kind='bar', ax=axes[1], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1].set_title('AUC åˆ†æ•¸æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('ç–¾ç—…é¡å‹')\n",
    "axes[1].set_ylabel('AUC')\n",
    "axes[1].set_ylim([0.5, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "\n",
    "# åœ¨æŸ±ç‹€åœ–ä¸ŠåŠ ä¸Šæ•¸å€¼æ¨™ç±¤\n",
    "for i, v in enumerate(all_metrics['auc']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../docs/experiments/model_performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… åœ–è¡¨å·²å„²å­˜è‡³: docs/experiments/model_performance_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å˜—è©¦ Random Forest æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"#\" + \" \"*15 + \"Random Forest æ¨¡å‹æ¸¬è©¦\" + \" \"*19 + \"#\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# ä½¿ç”¨ Random Forest é æ¸¬é«˜è¡€å£“\n",
    "rf_model_hp, rf_metrics_hp = train_and_evaluate_model(\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    y_train_hp, y_test_hp,\n",
    "    'Random Forest',\n",
    "    'é«˜è¡€å£“ (Hypertension)'\n",
    ")\n",
    "\n",
    "# æ¯”è¼ƒ Logistic Regression vs Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ¨¡å‹æ¯”è¼ƒç¸½çµ - é«˜è¡€å£“é æ¸¬\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  AUC: {metrics_hp['auc']:.3f}\")\n",
    "print(f\"  F1:  {metrics_hp['f1']:.3f}\")\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  AUC: {rf_metrics_hp['auc']:.3f}\")\n",
    "print(f\"  F1:  {rf_metrics_hp['f1']:.3f}\")\n",
    "print(f\"\\næ”¹å–„å¹…åº¦:\")\n",
    "print(f\"  AUC: {(rf_metrics_hp['auc'] - metrics_hp['auc'])*100:+.1f}%\")\n",
    "print(f\"  F1:  {(rf_metrics_hp['f1'] - metrics_hp['f1'])*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç‰¹å¾µé‡è¦æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–å¾— Random Forest çš„ç‰¹å¾µé‡è¦æ€§\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': rf_model_hp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# é¡¯ç¤ºå‰ 15 å€‹æœ€é‡è¦çš„ç‰¹å¾µ\n",
    "print(\"=\"*60)\n",
    "print(\"ç‰¹å¾µé‡è¦æ€§æ’å (Top 15)\")\n",
    "print(\"=\"*60)\n",
    "for i, row in feature_importance.head(15).iterrows():\n",
    "    print(f\"{row['feature']:20s} {row['importance']:.4f}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('é‡è¦æ€§åˆ†æ•¸')\n",
    "plt.title('ç‰¹å¾µé‡è¦æ€§æ’å - é«˜è¡€å£“é æ¸¬', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../docs/experiments/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… åœ–è¡¨å·²å„²å­˜è‡³: docs/experiments/feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç¸½çµ\n",
    "\n",
    "### ğŸ¯ å®Œæˆçš„å·¥ä½œ\n",
    "1. âœ… æˆåŠŸå»ºç«‹äº†ä¸‰é«˜ï¼ˆé«˜è¡€å£“ã€é«˜è¡€ç³–ã€é«˜è¡€è„‚ï¼‰çš„é æ¸¬æ¨¡å‹\n",
    "2. âœ… ä½¿ç”¨ T1 + T2 çš„è³‡æ–™é æ¸¬ T3 æ™‚çš„ç–¾ç—…ç‹€æ…‹\n",
    "3. âœ… æ¯”è¼ƒäº† Logistic Regression å’Œ Random Forest å…©ç¨®æ¨¡å‹\n",
    "4. âœ… åˆ†æäº†ç‰¹å¾µé‡è¦æ€§\n",
    "\n",
    "### ğŸ“Š æ¨¡å‹æ•ˆèƒ½\n",
    "- **Logistic Regression**: æä¾›äº†ç©©å®šçš„ baseline çµæœ\n",
    "- **Random Forest**: é€šå¸¸èƒ½ç²å¾—æ›´å¥½çš„é æ¸¬æ•ˆèƒ½\n",
    "- **æœ€é‡è¦çš„ç‰¹å¾µ**: é€šå¸¸æ˜¯ T2 æ™‚é–“é»çš„æ¸¬é‡å€¼å’Œ Î” è®ŠåŒ–é‡\n",
    "\n",
    "### ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "1. **å˜—è©¦ XGBoost**: é€šå¸¸èƒ½ç²å¾—æœ€ä½³æ•ˆèƒ½\n",
    "2. **è¶…åƒæ•¸èª¿å„ª**: ä½¿ç”¨ GridSearchCV æˆ– RandomizedSearchCV\n",
    "3. **ç‰¹å¾µå·¥ç¨‹**: å‰µå»ºäº¤äº’ç‰¹å¾µæˆ–å¤šé …å¼ç‰¹å¾µ\n",
    "4. **é›†æˆå­¸ç¿’**: çµåˆå¤šå€‹æ¨¡å‹çš„é æ¸¬çµæœ\n",
    "5. **æ™‚åºåˆ†æ**: è€ƒæ…®ä½¿ç”¨ LSTM ç­‰æ™‚åºæ¨¡å‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}