# 第四章 模型方法

本研究採用十一種預測模型，依方法論性質分為三大類別：傳統統計方法、機器學習方法，以及符號回歸。分類架構如下：

- **傳統統計方法**：Logistic Regression (LR)、Naive Bayes (NB)、Linear Discriminant Analysis (LDA)
- **機器學習方法**
  - 基於實例：K-Nearest Neighbors (KNN)
  - 樹狀模型：Decision Tree (DT)、Random Forest (RF)、XGBoost、LightGBM
  - 核方法：Support Vector Machine (SVM)
  - 神經網路：Multi-Layer Perceptron (MLP)
- **符號回歸**：PySR

傳統統計方法具有明確的數學假設與高度可解釋性，適合作為基準模型。機器學習方法從樹狀模型的離散分割、核方法的連續決策邊界，到神經網路的多層非線性轉換，逐步提升模型的表達能力。符號回歸則嘗試從資料中演化出可解釋的數學公式，兼顧預測能力與可解釋性。以下依序介紹各類模型的原理、選用原因與實作設定。

## 4.1 傳統統計方法

傳統統計方法具有明確的數學形式與統計假設，模型參數可直接解讀其物理意義。本研究選用三種具代表性的線性分類方法：Logistic Regression 為判別式模型，直接建模後驗機率；Naive Bayes 為生成式模型，透過貝氏定理估計類別機率；Linear Discriminant Analysis 則透過最大化類別間距進行分類。三者在理論上形成互補的比較基礎。

### 4.1.1 Logistic Regression (LR)

Logistic Regression（邏輯斯迴歸）是疾病預測研究中最常用的基準模型，屬於判別式模型（Discriminative Model），直接建模特徵與類別之間的條件機率。根據 Sun et al. (2017) 的系統性回顧，在 26 篇高血壓預測研究所涵蓋的 48 個模型中，Logistic Regression 佔 12 篇（25%）為最大宗，顯示其在疾病風險預測領域的主流地位。

**模型形式**：

LR 透過 sigmoid 函數將線性組合映射至 [0, 1] 區間，輸出疾病發生的機率：

$$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n)}} \tag{4-1}$$

其中 $\beta_i$ 為迴歸係數，$e^{\beta_i}$ 可直接解釋為勝算比（Odds Ratio），表示第 $i$ 個特徵每增加一個單位時，疾病風險的倍數變化。此特性使 LR 在臨床應用中具有高度可解釋性，醫療人員可直觀理解各風險因子的貢獻程度。

在 Sun et al. (2017) 回顧的研究中，LR 的 C-statistic（等同 AUC）多落在 0.72–0.85 之間，顯示即使在非線性關係存在的情境下，LR 仍能提供具競爭力的預測效能。然而，LR 假設特徵與對數勝算之間為線性關係，可能無法捕捉複雜的非線性交互作用。

**選用原因**：

- 可解釋性高：迴歸係數可直接解讀為風險因子的貢獻（Odds Ratio）
- 計算效率高：適合作為基準模型
- 支援 class_weight：可處理類別不平衡問題

**實作參數**：

- `solver`: 'lbfgs'
- `max_iter`: 1000
- `class_weight`: 'balanced'（處理類別不平衡）

### 4.1.2 Naive Bayes (NB)

Naive Bayes（單純貝氏分類器）是一種基於貝氏定理的生成式模型（Generative Model），透過估計各類別下特徵的聯合分佈進行分類。與 Logistic Regression 的判別式模型形成理論上的互補，兩者的比較有助於理解資料的分佈特性。

**演算法原理**：

NB 基於貝氏定理進行分類：

$$P(Y=k|X) = \frac{P(X|Y=k) \cdot P(Y=k)}{P(X)} \tag{4-2}$$

其核心假設為各特徵在給定類別下條件獨立，即：

$$P(X|Y=k) = \prod_{j=1}^{n} P(X_j|Y=k) \tag{4-3}$$

本研究使用 Gaussian Naive Bayes，假設連續特徵在各類別下服從常態分佈：

$$P(X_j|Y=k) = \frac{1}{\sqrt{2\pi\sigma_{jk}^2}} \exp\left(-\frac{(X_j - \mu_{jk})^2}{2\sigma_{jk}^2}\right) \tag{4-4}$$

此方法的計算效率極高（時間複雜度為 $O(nd)$，$n$ 為樣本數、$d$ 為特徵數），且無需迭代優化，在小樣本情境下表現穩健。然而，特徵獨立假設在醫療資料中往往不成立，例如收縮壓與舒張壓、血糖與 BMI 之間均存在相關性，此假設的違反可能影響機率估計的校準度，但對分類排序（AUC）的影響通常較小。

**選用原因**：

- 計算效率極高：僅需估計各特徵的均值與變異數
- 理論基礎明確：基於機率推論框架
- 適合作為基準：與 LR 的判別式模型形成互補比較

**實作參數**：

- 使用 scikit-learn 的 `GaussianNB`
- `priors`: None（依訓練資料自動估計類別先驗機率）

**注意**：Naive Bayes 的條件獨立假設在實務中通常不完全成立（如 SBP 與 DBP 高度相關），但即便假設違反，其分類表現仍可作為有意義的參考基準。

### 4.1.3 Linear Discriminant Analysis (LDA)

線性判別分析（Linear Discriminant Analysis, LDA）由 Fisher (1936) 提出，是統計學中最經典的分類方法之一。LDA 屬於生成式模型，透過最大化類別間變異與類別內變異的比值，尋找最佳線性投影方向進行分類。

**演算法原理**：

LDA 尋找投影方向 $w$，使得 Fisher 準則最大化：

$$J(w) = \frac{w^T S_B w}{w^T S_W w} \tag{4-5}$$

其中 $S_B$ 為類別間散布矩陣（Between-class scatter matrix），$S_W$ 為類別內散布矩陣（Within-class scatter matrix），$w$ 為投影方向。LDA 假設各類別的特徵服從多變量常態分佈且共享相同的共變異數矩陣。

相較於 Logistic Regression，LDA 同時考慮特徵的聯合分佈結構，在特徵間存在多重共線性時仍能維持穩定性。此外，LDA 的降維特性（將 $d$ 維特徵投影至最多 $k-1$ 維空間，$k$ 為類別數）使其在高維度資料中具有正則化效果。然而，常態分佈與等共變異數假設在實際資料中可能不完全成立，限制了其對非線性關係的捕捉能力。

**選用原因**：

- 兼具降維與分類功能：可同時降低特徵維度
- 考慮類別分佈結構：利用共變異數矩陣進行判別
- 計算效率高：無需迭代優化

**實作參數**：

- 使用 scikit-learn 的 `LinearDiscriminantAnalysis`
- `solver`: 'svd'（奇異值分解，適合特徵數多於樣本數的情況）
- `priors`: None（依訓練資料自動估計）

## 4.2 機器學習方法

相較於傳統統計方法的線性假設，機器學習方法能捕捉更複雜的非線性關係與特徵交互作用。本節依序介紹基於實例的方法（KNN）、樹狀模型（DT、RF、XGBoost、LightGBM）、核方法（SVM）與神經網路（MLP），四類方法在學習策略與模型複雜度上各有特色：基於實例的方法不建立顯式模型，直接利用訓練資料進行預測；樹狀模型透過離散分割建立規則；核方法在高維空間中建立連續決策邊界；神經網路則透過多層非線性轉換學習抽象的特徵表示。

### 4.2.1 K-Nearest Neighbors (KNN)

K-最近鄰（K-Nearest Neighbors, KNN）是一種基於實例的學習方法（Instance-based Learning），屬於惰性學習（Lazy Learning）——訓練階段不建立顯式模型，而是在預測時直接從訓練資料中搜尋最相似的樣本進行分類。

**演算法原理**：

給定一個待分類樣本 $x$，KNN 在訓練集中找出距離 $x$ 最近的 $k$ 個鄰居，以這 $k$ 個鄰居的多數類別作為預測結果：

$$\hat{y} = \arg\max_c \sum_{i \in N_k(x)} \mathbb{1}(y_i = c) \tag{4-6}$$

其中 $N_k(x)$ 為 $x$ 的 $k$ 個最近鄰集合，$\mathbb{1}$ 為指示函數。距離度量採用歐氏距離（Euclidean Distance）：

$$d(x, x') = \sqrt{\sum_{j=1}^{d} (x_j - x'_j)^2} \tag{4-7}$$

KNN 的預測品質高度依賴 $k$ 值的選擇與特徵的尺度。$k$ 值過小容易受雜訊影響，過大則可能模糊類別邊界。由於使用歐氏距離，不同量綱的特徵需進行標準化處理。

**選用原因**：

- 學習範式獨特：不建立參數模型，與其他方法形成對比
- 概念直觀：「相似的人有相似的健康風險」符合醫學直覺
- 實作簡便：scikit-learn 提供 `KNeighborsClassifier`

**實作參數**：

- `n_neighbors`: 5
- `metric`: 'minkowski'（p=2，即歐氏距離）
- `weights`: 'uniform'（等權投票）

**注意**：KNN 不直接支援 class_weight 參數。本研究在預測階段使用距離加權投票，並搭配標準化特徵以確保各維度的距離貢獻均等。

### 4.2.2 Decision Tree (DT)

決策樹是一種基於規則的分類模型，透過遞迴地將資料依特徵值分割成子集，最終形成樹狀結構。每個內部節點代表一個特徵的判斷條件，每個葉節點代表一個類別預測。

**演算法原理**：

決策樹的建構過程為：在每個節點選擇使資料純度最大化的特徵與閾值進行分割。本研究使用 Gini Impurity（基尼不純度）作為分割準則：

$$Gini(t) = 1 - \sum_{k=1}^{K} p_k^2 \tag{4-8}$$

其中 $p_k$ 為節點 $t$ 中第 $k$ 類樣本的比例。Gini 值越小代表節點越純，當節點中所有樣本屬於同一類別時，$Gini = 0$。在每次分割時，演算法選擇使加權 Gini 值下降最多的特徵與閾值：

$$\Delta Gini = Gini(parent) - \frac{n_{left}}{n} Gini(left) - \frac{n_{right}}{n} Gini(right) \tag{4-9}$$

分割過程遞迴執行，直到滿足停止條件（如深度限制或樣本數不足）。

**選用原因**：

- 高度可解釋：分類規則可直接呈現為 if-then 規則
- 計算效率高：訓練與預測速度快
- 支援 class_weight：可處理類別不平衡問題

**實作參數**：

- `criterion`: 'gini'
- `max_depth`: None（完全生長）
- `class_weight`: 'balanced'

**注意**：單一決策樹容易過擬合，預測效能通常低於集成方法，但因其高可解釋性，仍納入比較。

### 4.2.3 Random Forest (RF)

Random Forest（隨機森林）是由 Breiman (2001) 提出的集成學習方法，透過結合多棵決策樹的預測來提升效能與穩定性。其核心思想為：多個弱學習器（個別決策樹）透過集成可形成強學習器。

**演算法原理**：

Random Forest 基於 Bagging（Bootstrap Aggregating）策略：

1. **Bootstrap 抽樣**：從原始資料中有放回地抽樣產生 $B$ 個子資料集
2. **隨機特徵選取**：在每棵樹的每次分裂時，僅從隨機選取的 $m$ 個特徵（$m \ll d$，$d$ 為總特徵數）中選擇最佳分割
3. **集成預測**：最終預測為所有樹的多數決（分類）：

$$\hat{y} = \text{mode}\{h_1(x), h_2(x), \ldots, h_B(x)\} \tag{4-10}$$

其中 $h_b(x)$ 為第 $b$ 棵決策樹的預測結果。Bootstrap 抽樣引入的隨機性降低了模型的變異數，而隨機特徵選取進一步降低了樹之間的相關性，使集成效果更為顯著。

**選用原因**：

- 抗過擬合：Bagging 機制降低變異數
- 穩定性高：對異常值和雜訊較不敏感
- 支援 class_weight：可處理類別不平衡問題

**實作參數**：

- `n_estimators`: 100
- `max_depth`: None（完全生長）
- `class_weight`: 'balanced'

### 4.2.4 XGBoost

XGBoost（eXtreme Gradient Boosting）由 Chen & Guestrin (2016) 提出，是一種基於梯度提升（Gradient Boosting）的集成學習方法。與 Random Forest 的平行集成（Bagging）不同，XGBoost 採用序列式集成策略，每棵新樹專注於修正前面樹的預測誤差，在多項醫學預測任務中表現優異。

**演算法原理**：

梯度提升透過逐步加入決策樹，累積修正預測結果：

$$\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i) \tag{4-11}$$

其中 $f_t$ 為第 $t$ 棵新加入的決策樹。XGBoost 的目標函數同時考慮預測誤差與模型複雜度：

$$\mathcal{L}^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t)}) + \sum_{k=1}^{t} \Omega(f_k) \tag{4-12}$$

其中 $l$ 為損失函數，$\Omega(f_k) = \gamma T + \frac{1}{2}\lambda \|w\|^2$ 為正則化項（$T$ 為葉節點數，$w$ 為葉節點權重），用於控制模型複雜度、防止過擬合。

**選用原因**：

- 預測效能強：在許多醫學預測任務中表現優異
- 可處理非線性關係：能捕捉特徵間的複雜交互作用
- 內建正則化：目標函數含正則化項，有效防止過擬合
- 支援特徵重要性評估

**實作參數**：

- `n_estimators`: 100
- `max_depth`: 6
- `learning_rate`: 0.1
- `scale_pos_weight`: 自動計算（處理類別不平衡）

### 4.2.5 LightGBM

LightGBM（Light Gradient Boosting Machine）由 Ke et al. (2017) 提出，同屬梯度提升樹家族，但針對訓練效率進行了兩項關鍵優化，使其在大規模資料集上的訓練速度顯著快於 XGBoost，同時維持相近的預測效能。

**演算法原理**：

LightGBM 的梯度提升框架與 XGBoost 相同，但在樹的建構策略上引入兩項加速技術：

1. **基於梯度的單側採樣（Gradient-based One-Side Sampling, GOSS）**：在每次迭代中，保留梯度較大的樣本（對訓練貢獻較大），並從梯度較小的樣本中隨機抽樣，藉此減少訓練資料量而不損失太多資訊量。
2. **獨佔特徵綑綁（Exclusive Feature Bundling, EFB）**：將互斥的稀疏特徵合併為單一特徵，降低有效特徵維度，加速分裂點的搜尋。

此外，LightGBM 採用**逐葉生長（Leaf-wise）** 策略，每次選擇增益最大的葉節點進行分裂，相較於 XGBoost 的逐層生長（Level-wise），在相同葉節點數下通常能達到更低的損失值，但也更容易過擬合，需搭配深度限制。

**選用原因**：

- 訓練效率高：GOSS 與 EFB 加速策略使其適合大規模資料
- 與 XGBoost 互補：同為梯度提升但建構策略不同，可比較兩種策略的效能差異
- 支援類別不平衡處理：透過 `is_unbalance` 或 `scale_pos_weight` 參數

**實作參數**：

- `n_estimators`: 100
- `max_depth`: -1（不限制，搭配 `num_leaves` 控制複雜度）
- `num_leaves`: 31
- `learning_rate`: 0.1
- `is_unbalance`: True（處理類別不平衡）

### 4.2.6 Support Vector Machine (SVM)

支援向量機（Support Vector Machine, SVM）由 Cortes & Vapnik (1995) 提出，基於統計學習理論，透過尋找最大間隔超平面進行分類。相較於樹模型依賴特徵的離散分割，SVM 在特徵空間中建立連續的決策邊界，對小樣本與高維資料具有較好的泛化能力。本研究採用 SVM 作為核方法（Kernel Method）的代表，與線性方法（LR、NB、LDA）、樹模型（DT、RF、XGBoost）及神經網路（MLP）形成四類方法的完整比較架構。

**演算法原理**：

SVM 尋找一個超平面 $w^T x + b = 0$，使得兩類樣本之間的間隔（margin）最大化。此最佳化問題可表述為：

$$\min_{w, b} \frac{1}{2} \|w\|^2 \quad \text{subject to} \quad y_i(w^T x_i + b) \geq 1, \; \forall i \tag{4-13}$$

對於非線性可分的情況，引入軟間隔（Soft Margin），允許部分樣本落在間隔內或錯誤分類，並以參數 $C$ 控制分類錯誤的懲罰程度。

對於非線性問題，SVM 使用核函數（Kernel Function）將資料映射至高維空間。本研究採用徑向基函數（Radial Basis Function, RBF）核：

$$K(x_i, x_j) = \exp\left(-\gamma \|x_i - x_j\|^2\right) \tag{4-14}$$

RBF 核可隱式地將資料映射至無限維空間，使 SVM 能建立高度非線性的決策邊界。參數 $\gamma$ 控制核函數的作用範圍：$\gamma$ 越大，決策邊界越複雜。

**選用原因**：

- 理論基礎扎實：基於統計學習理論的結構風險最小化
- 適合中小型資料集：在樣本數有限時表現良好
- 支援 class_weight：可處理類別不平衡問題

**實作參數**：

- `kernel`: 'rbf'（徑向基函數）
- `C`: 1.0
- `gamma`: 'scale'
- `class_weight`: 'balanced'

### 4.2.7 Multi-Layer Perceptron (MLP)

多層感知器（Multi-Layer Perceptron, MLP）是一種前饋神經網路（Feedforward Neural Network），透過多層神經元的非線性轉換學習複雜的特徵表示。MLP 的理論基礎可追溯至 Rumelhart et al. (1986) 提出的反向傳播（Backpropagation）演算法，該演算法使得多層神經網路的訓練成為可能。本研究使用 scikit-learn 的 MLPClassifier 實作。

**演算法原理**：

MLP 由輸入層、隱藏層與輸出層組成。本研究的網路架構為：

- 輸入層：26 個特徵（對應 26 個健檢指標與變化量）
- 隱藏層：2 層，每層 64 個神經元
- 輸出層：1 個神經元（二元分類）

**前向傳播**（Forward Propagation）：資料從輸入層逐層向前傳遞，每一層的計算包含線性轉換與非線性激活：

$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \tag{4-15}$$
$$a^{(l)} = \sigma(z^{(l)}) \tag{4-16}$$

其中 $W^{(l)}$ 與 $b^{(l)}$ 分別為第 $l$ 層的權重矩陣與偏差向量，$a^{(l-1)}$ 為上一層的輸出（$a^{(0)} = X$ 為輸入特徵），$\sigma$ 為激活函數。

本研究採用修正線性單元（Rectified Linear Unit, ReLU）作為隱藏層的激活函數：

$$\text{ReLU}(z) = \max(0, z) \tag{4-17}$$

相較於傳統的 sigmoid 激活函數，ReLU 具有兩項優勢：（1）計算效率高，僅需比較與取最大值操作；（2）緩解梯度消失問題（Vanishing Gradient Problem）——sigmoid 在輸入絕對值較大時梯度趨近於零，導致深層網路的參數難以更新，而 ReLU 在正值區域梯度恆為 1，使梯度能有效地向後傳播。

**反向傳播與參數更新**：訓練時，反向傳播演算法（Rumelhart et al., 1986）利用鏈式法則（Chain Rule）計算損失函數對每一層參數的梯度，再透過優化器更新權重。本研究採用 Adam 優化器，其結合了動量（Momentum）與自適應學習率（Adaptive Learning Rate），在神經網路訓練中被廣泛使用，相較於基本的隨機梯度下降（Stochastic Gradient Descent, SGD），Adam 對學習率的初始設定較不敏感，收斂速度更快。

**選用原因**：

- 非線性建模：透過多層非線性轉換，可學習複雜的特徵交互
- 彈性高：可調整網路深度與寬度以適應不同複雜度的任務
- 實作簡便：scikit-learn 提供統一的 API 介面

**實作參數**：

- `hidden_layer_sizes`: (64, 64)
- `activation`: 'relu'
- `solver`: 'adam'
- `max_iter`: 500

**注意**：MLPClassifier 不直接支援 class_weight 參數，本研究透過手動調整樣本權重（sample_weight）處理類別不平衡問題。

## 4.3 符號回歸

### 4.3.1 PySR

符號回歸（Symbolic Regression）透過遺傳規劃（Genetic Programming, GP）演化出可解釋的數學公式（Cranmer, 2023）。相較於前述的黑盒模型，符號回歸產出的公式可直接理解其醫學意義，兼顧預測能力與可解釋性。

**演算法原理**：

遺傳規劃的核心流程為：

1. **初始化**：隨機生成一群數學公式（個體），每個公式為一棵運算樹
2. **適應度評估**：計算每個公式的預測誤差（如均方誤差）
3. **選擇**：保留表現較好的公式
4. **演化操作**：透過交叉（Crossover，交換子樹）與突變（Mutation，隨機修改節點）產生新公式
5. **迭代**：重複步驟 2-4 直到收斂

PySR 在搜尋過程中同時考慮公式的精度與複雜度，產出一系列 Pareto 前沿上的候選公式，供研究者依據領域知識選擇最適當的公式。

**選用原因**：

- 完全透明：產出的公式可直接理解
- 領域知識驗證：可檢驗公式是否符合醫學邏輯
- 輕量部署：簡單公式不需複雜運算資源

**使用套件**：本研究初期使用 Python 原生的 gplearn 套件進行符號回歸實驗，但因其不支援 class_weight 且搜尋效率有限，後改用基於 Julia 的 PySR 套件（Cranmer, 2023）。PySR 支援 sample_weight 且搜尋效能更佳，為本研究最終採用之符號回歸工具。

## 4.4 類別不平衡處理策略

由於三高疾病的患病率較低（高血壓 16.68%、高血糖 5.53%、高血脂 5.96%），本研究採用 class_weight='balanced' 作為主要的類別不平衡處理策略（原理與公式詳見第二章）。各模型的具體設定如下：

- **LR、DT、RF、SVM**：設定 `class_weight='balanced'`，由 scikit-learn 自動依類別頻率倒數計算權重
- **XGBoost**：設定 `scale_pos_weight` 為負正類比例，效果等同 balanced
- **LightGBM**：設定 `is_unbalance=True`，自動調整類別權重
- **MLP**：透過手動調整 `sample_weight` 實現加權訓練
- **NB、LDA、KNN**：不支援 class_weight，以原始資料分佈訓練
- **PySR**：透過 `sample_weight` 參數加權

**撰寫狀態**：章節重組完成
**最後更新**：2026-02-24
**維護者**：紀伯喬
