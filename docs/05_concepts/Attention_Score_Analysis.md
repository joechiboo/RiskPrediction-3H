# Attention Score åˆ†æå‚™å¿˜éŒ„

## ğŸ“Œ æ¦‚è¿°

Attention Scoreï¼ˆæ³¨æ„åŠ›åˆ†æ•¸ï¼‰æ˜¯æ·±åº¦å­¸ç¿’æ¨¡å‹ç”¨ä¾†è­˜åˆ¥**å“ªäº›è¼¸å…¥ç‰¹å¾µæœ€é‡è¦**çš„æ©Ÿåˆ¶ã€‚æœ¬æ–‡ä»¶èªªæ˜ Attention Score çš„åŸç†ã€æ‡‰ç”¨ï¼Œä»¥åŠåœ¨ä¸‰é«˜ç–¾ç—…é æ¸¬ç ”ç©¶ä¸­çš„é‹ç”¨ã€‚

---

## ğŸ¯ ä»€éº¼æ˜¯ Attention Scoreï¼Ÿ

### ç°¡å–®æ¯”å–»

æƒ³åƒé†«ç”Ÿçœ‹ç—…æ­·ï¼š
- ç—…æ­·æœ‰ 100 é è³‡æ–™
- é†«ç”Ÿæœƒ**ç‰¹åˆ¥æ³¨æ„**æŸäº›é—œéµè³‡è¨Šï¼ˆå¦‚è¡€å£“è¶¨å‹¢ã€å®¶æ—å²ï¼‰
- **Attention Score** å°±æ˜¯æ¨¡å‹å‘Šè¨´ä½ ï¼šã€Œæˆ‘åœ¨é€™ 100 å€‹ç‰¹å¾µä¸­ï¼Œæœ€é—œæ³¨å“ªå¹¾å€‹ã€

### æŠ€è¡“å®šç¾©

**Attention Score** = æ¨¡å‹å°æ¯å€‹è¼¸å…¥ç‰¹å¾µçš„é‡è¦æ€§è©•ä¼°

- **æ•¸å€¼ç¯„åœ**ï¼š0-1 ä¹‹é–“ï¼ˆç¶“é softmax æ¨™æº–åŒ–ï¼‰
- **æ•¸å€¼è¶Šé«˜**ï¼šè¡¨ç¤ºæ¨¡å‹èªç‚ºè©²ç‰¹å¾µå°é æ¸¬çµæœè¶Šé‡è¦
- **ç¸½å’Œç‚º 1**ï¼šæ‰€æœ‰ç‰¹å¾µçš„ Attention Score åŠ èµ·ä¾† = 1

### æ•¸å­¸å…¬å¼

```
Attention(Q, K, V) = softmax(Q Ã— K^T / âˆšd_k) Ã— V
```

**åƒæ•¸èªªæ˜**ï¼š
- **Q (Query)**ï¼šæŸ¥è©¢å‘é‡ï¼Œä»£è¡¨ã€Œæˆ‘è¦æ‰¾ä»€éº¼ã€
- **K (Key)**ï¼šéµå‘é‡ï¼Œä»£è¡¨ã€Œæ¯å€‹ç‰¹å¾µæ˜¯ä»€éº¼ã€
- **V (Value)**ï¼šå€¼å‘é‡ï¼Œä»£è¡¨ã€Œç‰¹å¾µçš„å¯¦éš›å…§å®¹ã€
- **d_k**ï¼šéµå‘é‡çš„ç¶­åº¦ï¼ˆç”¨æ–¼ç¸®æ”¾ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼‰

**è¨ˆç®—æ­¥é©Ÿ**ï¼š
1. è¨ˆç®—ç›¸ä¼¼åº¦ï¼š`Q Ã— K^T`ï¼ˆå“ªäº›ç‰¹å¾µèˆ‡æŸ¥è©¢ç›¸é—œï¼‰
2. ç¸®æ”¾ï¼š`/ âˆšd_k`ï¼ˆé˜²æ­¢æ•¸å€¼éå¤§ï¼‰
3. æ¨™æº–åŒ–ï¼š`softmax(...)`ï¼ˆè½‰æ›æˆ 0-1 çš„åˆ†æ•¸ï¼‰
4. åŠ æ¬Šæ±‚å’Œï¼š`Ã— V`ï¼ˆæ ¹æ“šé‡è¦æ€§åŠ æ¬Šç‰¹å¾µï¼‰

---

## ğŸ”¬ Taiwan MTL (2025) çš„ Attention åˆ†æ

### ç ”ç©¶èƒŒæ™¯

- **è«–æ–‡**ï¼šMultitask learning multimodal network for chronic disease prediction
- **ä½œè€…**ï¼šHsinhan Tsai et al., National Taiwan University
- **æœŸåˆŠ**ï¼šScientific Reports (2025)
- **DOI**ï¼š10.1038/s41598-025-99554-z

### æ¨¡å‹æ¶æ§‹ä¸­çš„ Attention

#### Multi-Head Self-Attention (MHSA) å±¤

```
è¼¸å…¥ï¼š180 å€‹ ICD ä»£ç¢¼ embedding
    â†“
MHSA è¨ˆç®—ç–¾ç—…ä¹‹é–“çš„ç›¸é—œæ€§
    â†“
è¼¸å‡ºï¼šåŠ æ¬Šå¾Œçš„ç–¾ç—…ç‰¹å¾µè¡¨ç¤º + Attention Scores
```

**è¶…åƒæ•¸è¨­å®š**ï¼š
- Number of heads: 1
- Key dimension: 16
- Value dimension: 8

**ä½œç”¨**ï¼š
- å­¸ç¿’å“ªäº›ç–¾ç—…ä¹‹é–“æœ‰é—œè¯
- ä¾‹å¦‚ï¼šé«˜è¡€è„‚ â†” å¿ƒè‡Ÿç—…ï¼ˆAttention Score é«˜ï¼‰
- ä¾‹å¦‚ï¼šç™½å…§éšœ â†” å¿ƒè‡Ÿç—…ï¼ˆAttention Score ä½ï¼‰

---

### Attention Score åˆ†ææµç¨‹

#### Step 1: é¸æ“‡é«˜é¢¨éšªæ‚£è€…

```
æ‰€æœ‰æ‚£è€… (555,124 äºº)
    â†“ ç¯©é¸æ¢ä»¶
é¸å–é æ¸¬åˆ†æ•¸æœ€é«˜çš„å‰ 2000 åæ‚£è€…
    â†“
é€™äº›æ˜¯æ¨¡å‹ã€Œæœ€æœ‰æŠŠæ¡ã€æœƒç™¼ç—…çš„äºº
```

**ç‚ºä»€éº¼é¸å‰ 2000 åï¼Ÿ**
- é«˜ç½®ä¿¡åº¦é æ¸¬ï¼Œæ¨¡å‹æœ€ç¢ºå®šçš„æ¡ˆä¾‹
- åˆ†æé€™ç¾¤äººçš„å…±åŒç–¾ç—…æ¨¡å¼
- æ‰¾å‡ºå“ªäº›ç–¾ç—…çµ„åˆæœ€å®¹æ˜“å°è‡´æ…¢æ€§ç—…

#### Step 2: è¨ˆç®— ICD é…å°çš„ Attention Score

**æ¯å€‹æ‚£è€…çš„é†«ç™‚ç´€éŒ„**ï¼š
```
æ‚£è€… Aï¼šéå» 10 å¹´ï¼Œ180 å€‹ ICD ä»£ç¢¼

æ¨¡å‹è¨ˆç®—æ¯å…©å€‹ ICD ä¹‹é–“çš„ Attention Scoreï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ICDâ‚ (é«˜è¡€è„‚) â†â†’ ICDâ‚‚ (ç³–å°¿ç—…)      â”‚
â”‚   Attention Score = 0.85            â”‚ â­ é«˜ç›¸é—œ
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ICDâ‚ (é«˜è¡€è„‚) â†â†’ ICDâ‚ƒ (ç™½å…§éšœ)      â”‚
â”‚   Attention Score = 0.12            â”‚ ä½ç›¸é—œ
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ICDâ‚‚ (ç³–å°¿ç—…) â†â†’ ICDâ‚„ (è…è¡°ç«­)      â”‚
â”‚   Attention Score = 0.91            â”‚ â­ è¶…é«˜ç›¸é—œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Step 3: çµ±è¨ˆé«˜ Attention Score çš„ ICD

åœ¨å‰ 2000 å€‹é«˜é¢¨éšªæ‚£è€…ä¸­ï¼š
- çµ±è¨ˆå“ªäº› ICD é…å°çš„ Attention Score æœ€å¸¸å‡ºç¾
- è­˜åˆ¥æ¨¡å‹èªç‚ºæœ€é‡è¦çš„ç–¾ç—…é—œè¯

---

### ç™¼ç¾çš„ä¸‰é¡é—œéµç–¾ç—…

#### âœ… é¡åˆ¥ 1ï¼šå¯ä¿®æ”¹é¢¨éšªå› å­

| ICD ä»£ç¢¼ | ç–¾ç—…åç¨± | Attention Score ç‰¹é» | é†«å­¸æ„ç¾© |
|---------|---------|---------------------|---------|
| 272.0 | ç´”é«˜è†½å›ºé†‡è¡€ç—‡ | ç¶“å¸¸èˆ‡æ…¢æ€§ç—…é«˜ç›¸é—œ | é«˜è¡€è„‚æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…ã€ç³–å°¿ç—…çš„é‡è¦é¢¨éšªå› å­ |
| 272.2 | æ··åˆå‹é«˜è¡€è„‚ | åœ¨é«˜é¢¨éšªæ‚£è€…ä¸­é »ç¹å‡ºç¾ | æ–‡ç»è­‰å¯¦ï¼šå¯é€éé£²é£Ÿã€è—¥ç‰©æ§åˆ¶ |
| 272.4 | å…¶ä»–é«˜è¡€è„‚ | èˆ‡å¤šç¨®æ…¢æ€§ç—…é—œè¯ | **é€™æ˜¯å¯æ”¹è®Šçš„ï¼** å¹²é é‡é» |

**é—œéµç™¼ç¾**ï¼š
- Attention æ©Ÿåˆ¶è‡ªå‹•è­˜åˆ¥å‡º**é«˜è¡€è„‚æ˜¯æ…¢æ€§ç—…çš„é‡è¦å‰å…†**
- èˆ‡é†«å­¸æ–‡ç»å®Œå…¨ä¸€è‡´ âœ…
- è­‰æ˜æ¨¡å‹å­¸åˆ°äº†çœŸå¯¦çš„é†«å­¸çŸ¥è­˜

---

#### ğŸ¥ é¡åˆ¥ 2ï¼šå¤šé‡æ…¢æ€§ç—…ï¼ˆMultimorbidityï¼‰

| ICD ä»£ç¢¼ | ç–¾ç—…åç¨± | ç‚ºä»€éº¼ Attention Score é«˜ï¼Ÿ |
|---------|---------|---------------------------|
| 274.9 | ç—›é¢¨ | èˆ‡ä»£è¬ç—‡å€™ç¾¤ç›¸é—œï¼Œè€å¹´äººå¸¸è¦‹ |
| 366.10 | è€å¹´æ€§ç™½å…§éšœ | è€åŒ–ç›¸é—œç–¾ç—…ï¼Œé«˜é½¡æ‚£è€…å¸¸è¦‹ |
| 571.40 | æ…¢æ€§è‚ç‚ | æ…¢æ€§ç—…æ‚£è€…å®¹æ˜“ä½µç™¼å¤šç¨®ç–¾ç—… |
| 585 | æ…¢æ€§è…è¡°ç«­ | ç³–å°¿ç—…ã€é«˜è¡€å£“çš„å¸¸è¦‹ä½µç™¼ç—‡ |
| 715.36 | ä¸‹è‚¢éª¨é—œç¯€ç‚ | è€å¹´æ…¢æ€§ç—…ï¼Œèˆ‡æ´»å‹•é‡ã€ä»£è¬ç›¸é—œ |
| 715.90 | éª¨é—œç¯€ç‚ï¼ˆæœªæ˜ç¤ºï¼‰ | åŒä¸Š |
| 721.3 | è…°è–¦æ¤è„Šæ¤ç—… | è€å¹´é€€åŒ–æ€§ç–¾ç—… |

**é†«å­¸æ„ç¾©**ï¼š
- æ‚£æœ‰**ä¸€ç¨®æ…¢æ€§ç—…**çš„äººï¼Œå®¹æ˜“ç™¼å±•**å¤šç¨®æ…¢æ€§ç—…**
- æ–‡ç»è­‰å¯¦ï¼šMultimorbidityï¼ˆå¤šé‡æ…¢æ€§ç—…ï¼‰åœ¨è€å¹´äººä¸­æ™®é
- Attention æ©Ÿåˆ¶æ•æ‰åˆ°äº†é€™å€‹é‡è¦çš„å…±ç—…æ¨¡å¼ âœ…

**å¯¦å‹™å•Ÿç¤º**ï¼š
- å¦‚æœæ‚£è€…å·²æœ‰ä¸€ç¨®æ…¢æ€§ç—… â†’ éœ€è¦æ›´å¯†åˆ‡ç›£æ§å…¶ä»–æ…¢æ€§ç—…é¢¨éšª
- é é˜²ç­–ç•¥æ‡‰è©²è€ƒæ…®å¤šé‡ç–¾ç—…ç®¡ç†

---

#### ğŸ†• é¡åˆ¥ 3ï¼šæ–°èˆˆé¢¨éšªå› å­

| ICD ä»£ç¢¼ | ç–¾ç—…åç¨± | æ–°ç™¼ç¾ï¼ | è¿‘æœŸç ”ç©¶è­‰æ“š |
|---------|---------|---------|-------------|
| 300.00 | ç„¦æ…®ç—‡ | ç²¾ç¥å¥åº· â†’ æ…¢æ€§ç—…é¢¨éšª â†‘ | å£“åŠ›å½±éŸ¿å…ç–«ç³»çµ±ã€å…§åˆ†æ³Œ |
| 300.4 | ç¥ç¶“æ€§æ†‚é¬±ç—‡ | æ†‚é¬± â†’ ç”Ÿæ´»ç¿’æ…£æƒ¡åŒ– â†’ ç–¾ç—…ç™¼ç”Ÿ | æ†‚é¬±æ‚£è€…é‹å‹•å°‘ã€é£²é£Ÿå·® |

**é†«å­¸æ„ç¾©**ï¼š
- é€™æ˜¯**è¼ƒæ–°çš„ç ”ç©¶æ–¹å‘**ï¼ˆéå»è¼ƒå°‘è¢«é—œæ³¨ï¼‰
- è¿‘æœŸç ”ç©¶æ‰é–‹å§‹æ¢è¨ï¼šç²¾ç¥å¥åº· â†” æ…¢æ€§ç—…çš„é›™å‘é—œä¿‚
- Attention æ©Ÿåˆ¶ç™¼ç¾äº†æ–‡ç»ä¸­æ‰å‰›é–‹å§‹æ¢è¨çš„é—œè¯ âœ¨

**è«–æ–‡è©•è«–**ï¼š
> "é€™äº›ç™¼ç¾ä½¿ç„¦æ…®å’Œæ†‚é¬±æˆç‚ºå€¼å¾—é€²ä¸€æ­¥èª¿æŸ¥çš„æ½›åœ¨é¢¨éšªå› å­"

**å¯¦å‹™å•Ÿç¤º**ï¼š
- æ…¢æ€§ç—…é é˜²æ‡‰è©²ç´å…¥**å¿ƒç†å¥åº·ç¯©æª¢**
- å£“åŠ›ç®¡ç†å¯èƒ½æ˜¯é é˜²æ…¢æ€§ç—…çš„é‡è¦ç­–ç•¥

---

## ğŸ“Š Attention Score å¯è¦–åŒ–ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šå–®ä¸€æ‚£è€…çš„ Attention Heatmap

å‡è¨­**æ‚£è€… X**è¢«é æ¸¬ç‚ºé«˜é¢¨éšªï¼Œåˆ†æéå» 10 å¹´çš„é†«ç™‚ç´€éŒ„ï¼š

```
æ‚£è€… X çš„é†«ç™‚ç´€éŒ„ Attention Scoreï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2014å¹´ï¼šé«˜è¡€è„‚ (272.2)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.85  â”‚ â­ æœ€é«˜
â”‚ 2015å¹´ï¼šç—›é¢¨ (274.9)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    0.62  â”‚
â”‚ 2016å¹´ï¼šé«˜è¡€è„‚ (272.0)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   0.78  â”‚
â”‚ 2017å¹´ï¼šç„¦æ…®ç—‡ (300.0)   â–ˆâ–ˆâ–ˆâ–ˆ      0.45  â”‚
â”‚ 2018å¹´ï¼šé—œç¯€ç‚ (715.3)   â–ˆâ–ˆâ–ˆ       0.38  â”‚
â”‚ 2019å¹´ï¼šç™½å…§éšœ (366.1)   â–ˆâ–ˆ        0.28  â”‚
â”‚ 2020å¹´ï¼šæ…¢æ€§è‚ç‚ (571.4) â–ˆâ–ˆ        0.25  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Attention Score (0-1)
```

**æ¨¡å‹çš„æ€è€ƒéç¨‹**ï¼š
> "é€™å€‹æ‚£è€…æœ‰å…©æ¬¡é«˜è¡€è„‚è¨ºæ–·ï¼ˆ272.2, 272.0ï¼‰ï¼ŒAttention Score å¾ˆé«˜ï¼ˆ0.85, 0.78ï¼‰
>
> â†’ æˆ‘èªç‚º**é«˜è¡€è„‚æ˜¯æœ€é‡è¦çš„é¢¨éšªä¿¡è™Ÿ**
>
> â†’ åŠ ä¸Šç—›é¢¨ï¼ˆä»£è¬å•é¡Œï¼‰ï¼Œç„¦æ…®ç—‡ï¼ˆå£“åŠ›ï¼‰
>
> â†’ ç¶œåˆåˆ¤æ–·ï¼šæœªä¾† 5 å¹´ç™¼å±•ç³–å°¿ç—…/å¿ƒè‡Ÿç—…çš„æ©Ÿç‡å¾ˆé«˜"

---

### ç¯„ä¾‹ 2ï¼šå¤šæ‚£è€…çš„ ICD é—œè¯ç†±åŠ›åœ–

åœ¨ 2000 å€‹é«˜é¢¨éšªæ‚£è€…ä¸­ï¼Œçµ±è¨ˆ ICD é…å°çš„å¹³å‡ Attention Scoreï¼š

```
        ç³–å°¿ç—…  å¿ƒè‡Ÿç—…  é«˜è¡€å£“  é«˜è¡€è„‚  è…è¡°ç«­
ç³–å°¿ç—…   1.00   0.78   0.82   0.85   0.71
å¿ƒè‡Ÿç—…   0.78   1.00   0.88   0.79   0.65
é«˜è¡€å£“   0.82   0.88   1.00   0.76   0.69
é«˜è¡€è„‚   0.85   0.79   0.76   1.00   0.58
è…è¡°ç«­   0.71   0.65   0.69   0.58   1.00

ğŸ”´ æ·±ç´… (0.8-1.0)ï¼šå¼·ç›¸é—œ
ğŸŸ  æ©™è‰² (0.6-0.8)ï¼šä¸­ç­‰ç›¸é—œ
âšª ç™½è‰² (0.0-0.6)ï¼šå¼±ç›¸é—œ
```

**é—œéµç™¼ç¾**ï¼š
- **ç³–å°¿ç—… â†” é«˜è¡€è„‚**ï¼šAttention Score = 0.85ï¼ˆæœ€å¼·ï¼‰
- **é«˜è¡€å£“ â†” å¿ƒè‡Ÿç—…**ï¼šAttention Score = 0.88ï¼ˆè¶…å¼·ï¼‰
- **ç³–å°¿ç—… â†” è…è¡°ç«­**ï¼šAttention Score = 0.71ï¼ˆå¸¸è¦‹ä½µç™¼ç—‡ï¼‰

---

## ğŸ†š Attention Score vs SHAP Value

### æ ¸å¿ƒå·®ç•°

| æ¯”è¼ƒé …ç›® | Attention Score | SHAP Value |
|---------|----------------|------------|
| **è¨ˆç®—æ™‚æ©Ÿ** | æ¨¡å‹è¨“ç·´æ™‚å­¸ç¿’ | æ¨¡å‹è¨“ç·´å¾Œè¨ˆç®— |
| **ç†è«–åŸºç¤** | ç¥ç¶“ç¶²è·¯å¯å­¸ç¿’æ¬Šé‡ | è³½å±€ç†è«– (Shapley Value) |
| **é©ç”¨æ¨¡å‹** | åƒ…ç¥ç¶“ç¶²è·¯ï¼ˆéœ€ Attention å±¤ï¼‰ | æ‰€æœ‰æ¨¡å‹ï¼ˆmodel-agnosticï¼‰ |
| **è¼¸å‡ºå«ç¾©** | ç›¸å°é‡è¦æ€§ (0-1) | ç‰¹å¾µè²¢ç»å€¼ï¼ˆå¯æ­£å¯è² ï¼‰ |
| **è¨ˆç®—æˆæœ¬** | ä½ï¼ˆæ¨è«–æ™‚ç›´æ¥å¾—åˆ°ï¼‰ | é«˜ï¼ˆéœ€é‡è¤‡è¨ˆç®—ï¼‰ |
| **ç†è«–ä¿è­‰** | ç„¡ | æœ‰ï¼ˆæ»¿è¶³å…¬å¹³æ€§ç­‰å…¬ç†ï¼‰ |
| **è¦–è¦ºåŒ–** | ç†±åŠ›åœ–ã€æ¢å½¢åœ– | ç€‘å¸ƒåœ–ã€èœ‚ç¾¤åœ–ã€åŠ›åœ– |
| **å¯æ¯”è¼ƒæ€§** | ç„¡æ³•è·¨æ¨¡å‹æ¯”è¼ƒ | å¯è·¨æ¨¡å‹æ¯”è¼ƒï¼ˆLR, RF, NNï¼‰ |

---

### ä½•æ™‚ä½¿ç”¨ï¼Ÿ

#### âœ… ä½¿ç”¨ Attention Score

é©ç”¨æƒ…å¢ƒï¼š
- æƒ³è¦å±•ç¤ºç¥ç¶“ç¶²è·¯çš„ã€Œå…§å»ºå¯è§£é‡‹æ€§ã€
- éœ€è¦**å³æ™‚è§£é‡‹**ï¼ˆæ¨è«–æ™‚ç›´æ¥è¼¸å‡ºï¼Œç„¡éœ€é¡å¤–è¨ˆç®—ï¼‰
- åˆ†æ**ä¸åŒä»»å‹™**é—œæ³¨çš„ç‰¹å¾µå·®ç•°ï¼ˆMTL å ´æ™¯ï¼‰
- ç ”ç©¶**ç‰¹å¾µä¹‹é–“çš„é—œè¯**ï¼ˆå¦‚ ICD é…å°ï¼‰

å„ªé»ï¼š
```python
# æ¨è«–æ™‚ç›´æ¥å¾—åˆ°ï¼Œé€Ÿåº¦å¿«
predictions, attention_scores = model(patient_data)
print(attention_scores)  # ç«‹å³çŸ¥é“å“ªäº›ç‰¹å¾µé‡è¦
```

ç¼ºé»ï¼š
- åªèƒ½ç”¨åœ¨æœ‰ Attention æ©Ÿåˆ¶çš„ç¥ç¶“ç¶²è·¯
- ç„¡æ³•èˆ‡å…¶ä»–æ¨¡å‹ï¼ˆLRã€RFï¼‰æ¯”è¼ƒ

---

#### âœ… ä½¿ç”¨ SHAP Value

é©ç”¨æƒ…å¢ƒï¼š
- éœ€è¦**å…¬å¹³æ¯”è¼ƒä¸åŒæ¨¡å‹**ï¼ˆLR, RF, SVM, NNï¼‰
- è¦æ±‚ç†è«–ä¿è­‰å’Œæ•¸å­¸åš´è¬¹æ€§
- åˆ†æ**å–®ä¸€æ¨£æœ¬**çš„è©³ç´°è²¢ç»
- è§£é‡‹ç‚ºä»€éº¼æ¨¡å‹åšå‡ºæŸå€‹é æ¸¬

å„ªé»ï¼š
```python
# å¯ä»¥æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹
shap_lr = explain_model(logistic_regression, X_test)
shap_rf = explain_model(random_forest, X_test)
shap_nn = explain_model(neural_network, X_test)

# å…¬å¹³æ¯”è¼ƒï¼šLR vs RF vs NN çš„ç‰¹å¾µé‡è¦æ€§
```

ç¼ºé»ï¼š
- è¨ˆç®—æˆæœ¬é«˜ï¼ˆéœ€è¦é‡è¤‡æ¨è«–å¾ˆå¤šæ¬¡ï¼‰
- æ·±åº¦ç¥ç¶“ç¶²è·¯è¨ˆç®—ç‰¹åˆ¥æ…¢

---

### å»ºè­°ï¼šå…©è€…éƒ½ç”¨ ğŸ¯

**ç ”ç©¶ç­–ç•¥**ï¼š
1. **SHAP ç”¨æ–¼ä¸»è¦æ¨¡å‹æ¯”è¼ƒ**
   - å›ç­”ï¼šInterpretable vs Black-box å“ªå€‹å¥½ï¼Ÿ
   - å…¬å¹³æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹çš„å¯è§£é‡‹æ€§

2. **Attention ç”¨æ–¼å±•ç¤ºç¥ç¶“ç¶²è·¯çš„é¡å¤–å„ªå‹¢**
   - è­‰æ˜ï¼šç¥ç¶“ç¶²è·¯ä¸æ˜¯å®Œå…¨é»‘ç›’
   - å±•ç¤ºï¼šå…§å»ºå¯è§£é‡‹æ€§ + å³æ™‚è§£é‡‹èƒ½åŠ›

---

## ğŸ“ åœ¨ä¸‰é«˜é æ¸¬ç ”ç©¶ä¸­çš„æ‡‰ç”¨

### Taiwan MTL vs æˆ‘å€‘çš„ç ”ç©¶

| é …ç›® | Taiwan MTL (2025) | æˆ‘å€‘çš„ç ”ç©¶ï¼ˆè¦åŠƒï¼‰ |
|------|-------------------|-------------------|
| **é æ¸¬ç›®æ¨™** | ç³–å°¿ç—…ã€å¿ƒè‡Ÿç—…ã€ä¸­é¢¨ã€é«˜è¡€å£“ï¼ˆ4 ç¨®ï¼‰ | é«˜è¡€ç³–ã€é«˜è¡€å£“ã€é«˜è¡€è„‚ï¼ˆ3 ç¨®ä¸‰é«˜ï¼‰ |
| **è¼¸å…¥è³‡æ–™** | ICD è¨ºæ–·ä»£ç¢¼åºåˆ—ï¼ˆ10å¹´ï¼Œ180å€‹ï¼‰ | HRS èª¿æŸ¥è³‡æ–™ï¼ˆå•å· + ç”Ÿç‰©æ¨™è¨˜ï¼‰ |
| **ç‰¹å¾µé¡å‹** | é†«ç™‚ç´€éŒ„ + å€‹äººè³‡è¨Š | äººå£çµ±è¨ˆ + ç”Ÿæ´»æ–¹å¼ + ç”Ÿç†æŒ‡æ¨™ |
| **æ™‚åºçµæ§‹** | æœ‰ï¼ˆæ¯ 2 å€‹æœˆçš„ ICD åºåˆ—ï¼‰ | æœ‰ï¼ˆY-2 â†’ Y-1 â†’ Y æ™‚é–“é»ï¼‰ |
| **è³‡æ–™ä¾†æº** | å°ç£å¥ä¿è³‡æ–™åº« HWDC | Health and Retirement Study |
| **æ¨£æœ¬æ•¸** | 555,124 | å¾…ç¢ºèªï¼ˆç´„ 1,155 æœ‰ 3+ æ¬¡è¨˜éŒ„ï¼‰ |
| **MTL æ¶æ§‹** | Hard Parameter Sharing | å¯åƒè€ƒ |
| **Attention** | Multi-Head Self-Attention | å¯å€Ÿé‘‘æ¦‚å¿µ |

---

### é—œéµå·®ç•°ï¼šä¸èƒ½ç›´æ¥ä½¿ç”¨ ICD Embedding

**Taiwan MTL çš„æ–¹æ³•**ï¼š
```
ICD ä»£ç¢¼åºåˆ—ï¼š[250.0, 401.9, 272.2, ...]
    â†“
Word2Vec Embeddingï¼ˆå°‡ç›¸ä¼¼ç–¾ç—…åˆ†çµ„ï¼‰
    â†“
180 å€‹ç–¾ç—… embedding å‘é‡
    â†“
Multi-Head Self-Attentionï¼ˆå­¸ç¿’ç–¾ç—…é—œè¯ï¼‰
```

**æˆ‘å€‘çš„è³‡æ–™**ï¼š
```
HRS å•å·è³‡æ–™ï¼š
- è¡€å£“ï¼š135 mmHg
- è¡€ç³–ï¼š110 mg/dL
- BMIï¼š27
- å¹´é½¡ï¼š65 æ­²
...ï¼ˆæ²’æœ‰ ICD ä»£ç¢¼ï¼ï¼‰
```

**ç‚ºä»€éº¼ä¸èƒ½ç›´æ¥ç”¨ï¼Ÿ**
- âŒ ä»–å€‘ï¼šæœ‰ ICD è¨ºæ–·ä»£ç¢¼åºåˆ— â†’ å¯ç”¨ Word2Vec åšç–¾ç—… embedding
- âŒ æˆ‘å€‘ï¼šæ²’æœ‰ ICD ä»£ç¢¼ï¼Œåªæœ‰æ•¸å€¼å‹å•å·è®Šæ•¸ â†’ **ä¸é©ç”¨ ICD embedding**

---

### å¯ä»¥å€Ÿé‘‘çš„éƒ¨åˆ† âœ…

é›–ç„¶ä¸èƒ½ç›´æ¥ç”¨ ICD embeddingï¼Œä½†å¯ä»¥å€Ÿé‘‘ä»¥ä¸‹æ¦‚å¿µï¼š

#### 1. Multi-Task Learning æ¶æ§‹

```python
# åŒæ™‚é æ¸¬ä¸‰é«˜ï¼ˆè€Œéå»ºç«‹ä¸‰å€‹ç¨ç«‹æ¨¡å‹ï¼‰
class ThreeHighMTL(nn.Module):
    def __init__(self):
        self.shared_layers = nn.Sequential(...)  # å…±äº«å±¤
        self.hypertension_head = nn.Linear(...)   # é«˜è¡€å£“
        self.hyperglycemia_head = nn.Linear(...)  # é«˜è¡€ç³–
        self.dyslipidemia_head = nn.Linear(...)   # é«˜è¡€è„‚
```

**å¥½è™•**ï¼š
- æ•æ‰ä¸‰é«˜ä¹‹é–“çš„ç›¸é—œæ€§
- åƒæ•¸æ•ˆç‡é«˜ï¼ˆ1/3 åƒæ•¸é‡ï¼‰

---

#### 2. Feature-level Attention

é›–ç„¶æ²’æœ‰ ICD åºåˆ—ï¼Œä½†å¯ä»¥å°**æ•¸å€¼å‹ç‰¹å¾µ**ä½¿ç”¨ Attentionï¼š

```python
class FeatureAttention(nn.Module):
    """ç‰¹å¾µå±¤ç´šçš„ Attention"""
    def __init__(self, num_features):
        super().__init__()
        self.attention_weights = nn.Linear(num_features, num_features)

    def forward(self, x):
        # x: [batch_size, num_features]
        scores = self.attention_weights(x)
        attention = torch.softmax(scores, dim=1)
        weighted_features = x * attention
        return weighted_features, attention
```

**æ‡‰ç”¨ç¯„ä¾‹**ï¼š
```
è¼¸å…¥ç‰¹å¾µï¼ˆ28 å€‹ï¼‰ï¼š
- Y-2 ç‰¹å¾µï¼ˆ10 å€‹ï¼‰ï¼šSBP, DBP, BMI, Glucose, ...
- Y-1 ç‰¹å¾µï¼ˆ10 å€‹ï¼‰ï¼šSBP, DBP, BMI, Glucose, ...
- Î” ç‰¹å¾µï¼ˆ8 å€‹ï¼‰ï¼šÎ”SBP, Î”DBP, Î”BMI, ...
    â†“
Feature Attentionï¼ˆå­¸ç¿’å“ªäº›ç‰¹å¾µæœ€é‡è¦ï¼‰
    â†“
Attention Scoreï¼š
  - Î”SBP: 0.85 â­ æœ€é‡è¦
  - Y-1_SBP: 0.72
  - å¹´é½¡: 0.68
  - ...
```

---

#### 3. Task-specific Attentionï¼ˆå¤šä»»å‹™ Attentionï¼‰

æ¯å€‹ä»»å‹™ï¼ˆé«˜è¡€å£“ã€é«˜è¡€ç³–ã€é«˜è¡€è„‚ï¼‰å¯èƒ½é—œæ³¨ä¸åŒçš„ç‰¹å¾µï¼š

```python
class TaskSpecificAttention(nn.Module):
    """æ¯å€‹ä»»å‹™æœ‰è‡ªå·±çš„ Attention æ¬Šé‡"""
    def __init__(self, num_features, num_tasks=3):
        super().__init__()
        # æ¯å€‹ä»»å‹™ä¸€å€‹ Attention å±¤
        self.task_attentions = nn.ModuleList([
            nn.Linear(num_features, 1) for _ in range(num_tasks)
        ])

    def forward(self, x, task_id):
        # æ ¹æ“šä»»å‹™é¸æ“‡å°æ‡‰çš„ Attention
        scores = self.task_attentions[task_id](x)
        attention = torch.softmax(scores, dim=1)
        return x * attention, attention
```

**é æœŸç™¼ç¾**ï¼š
```
é«˜è¡€å£“ä»»å‹™çš„ Attentionï¼š
  - SBP, DBP â†’ é«˜åˆ†
  - Glucose â†’ ä½åˆ†

é«˜è¡€ç³–ä»»å‹™çš„ Attentionï¼š
  - Glucose, HbA1c â†’ é«˜åˆ†
  - SBP â†’ ä¸­ç­‰åˆ†ï¼ˆç›¸é—œä½†ä¸æ˜¯ä¸»å› ï¼‰

é«˜è¡€è„‚ä»»å‹™çš„ Attentionï¼š
  - Cholesterol, TG, LDL â†’ é«˜åˆ†
  - BMI â†’ ä¸­ç­‰åˆ†
```

---

#### 4. Temporal Attentionï¼ˆæ™‚åº Attentionï¼‰

æˆ‘å€‘æœ‰ Y-2 â†’ Y-1 â†’ Y çš„æ™‚åºè³‡æ–™ï¼Œå¯ä»¥ç”¨ Attention å­¸ç¿’å“ªå€‹æ™‚é–“é»æœ€é‡è¦ï¼š

```python
class TemporalAttention(nn.Module):
    """æ™‚é–“é»çš„ Attention"""
    def __init__(self, feature_dim):
        super().__init__()
        self.temporal_attention = nn.Linear(feature_dim, 1)

    def forward(self, t1_features, t2_features):
        # Stack: [batch_size, 2, feature_dim]
        temporal_features = torch.stack([t1_features, t2_features], dim=1)

        # è¨ˆç®—æ¯å€‹æ™‚é–“é»çš„é‡è¦æ€§
        scores = self.temporal_attention(temporal_features)
        attention = torch.softmax(scores, dim=1)

        # åŠ æ¬Šæ±‚å’Œ
        weighted = (temporal_features * attention).sum(dim=1)
        return weighted, attention
```

**æ‡‰ç”¨**ï¼š
```
æ‚£è€… Aï¼š
- Y-2 Attention: 0.35
- Y-1 Attention: 0.65 â­
â†’ æ¨¡å‹èªç‚ºã€Œæœ€è¿‘çš„æª¢æŸ¥ã€æ›´é‡è¦

æ‚£è€… Bï¼š
- Y-2 Attention: 0.72 â­
- Y-1 Attention: 0.28
â†’ æ¨¡å‹èªç‚ºã€Œæ—©æœŸç•°å¸¸ã€æ˜¯è­¦è¨Šï¼ˆå³ä½¿ Y-1 æ”¹å–„ï¼‰
```

---

## ğŸ’¡ å¯¦ä½œå»ºè­°

### å»ºè­°çš„æ¨¡å‹æ¶æ§‹ï¼ˆå®Œæ•´ï¼‰

```python
class ThreeHighPredictionWithAttention(nn.Module):
    def __init__(self, num_features=28, num_tasks=3):
        super().__init__()

        # 1. Feature-level Attention
        self.feature_attention = FeatureAttention(num_features)

        # 2. Shared layers (MTL)
        self.shared_layers = nn.Sequential(
            nn.Linear(num_features, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU()
        )

        # 3. Task-specific heads with Attention
        self.hypertension_attention = nn.Linear(64, 1)
        self.hyperglycemia_attention = nn.Linear(64, 1)
        self.dyslipidemia_attention = nn.Linear(64, 1)

        self.hypertension_head = nn.Linear(64, 1)
        self.hyperglycemia_head = nn.Linear(64, 1)
        self.dyslipidemia_head = nn.Linear(64, 1)

    def forward(self, x):
        # Feature Attention
        x_attended, feature_scores = self.feature_attention(x)

        # Shared representation
        shared_repr = self.shared_layers(x_attended)

        # Task-specific predictions with Attention
        hyp_attn = torch.sigmoid(self.hypertension_attention(shared_repr))
        glu_attn = torch.sigmoid(self.hyperglycemia_attention(shared_repr))
        lip_attn = torch.sigmoid(self.dyslipidemia_attention(shared_repr))

        hypertension_pred = torch.sigmoid(self.hypertension_head(shared_repr * hyp_attn))
        hyperglycemia_pred = torch.sigmoid(self.hyperglycemia_head(shared_repr * glu_attn))
        dyslipidemia_pred = torch.sigmoid(self.dyslipidemia_head(shared_repr * lip_attn))

        return {
            'predictions': {
                'hypertension': hypertension_pred,
                'hyperglycemia': hyperglycemia_pred,
                'dyslipidemia': dyslipidemia_pred
            },
            'attention_scores': {
                'feature_attention': feature_scores,
                'task_attention': {
                    'hypertension': hyp_attn,
                    'hyperglycemia': glu_attn,
                    'dyslipidemia': lip_attn
                }
            }
        }
```

---

### åˆ†æ Attention Score çš„æ­¥é©Ÿ

#### Step 1: è¨“ç·´æ¨¡å‹

```python
model = ThreeHighPredictionWithAttention(num_features=28)
# è¨“ç·´éç¨‹...
```

#### Step 2: æå–é«˜é¢¨éšªæ‚£è€…çš„ Attention

```python
# åœ¨æ¸¬è©¦é›†ä¸Šæ¨è«–
results = model(X_test)

# é¸å–é«˜é¢¨éšªæ‚£è€…ï¼ˆé æ¸¬åˆ†æ•¸ > 0.8ï¼‰
high_risk_mask = results['predictions']['hypertension'] > 0.8
high_risk_patients = X_test[high_risk_mask]
high_risk_attention = results['attention_scores']['feature_attention'][high_risk_mask]

# å¹³å‡ Attention Score
mean_attention = high_risk_attention.mean(dim=0)
```

#### Step 3: è¦–è¦ºåŒ– Feature Importance

```python
import matplotlib.pyplot as plt

feature_names = ['SBP_Y-2', 'DBP_Y-2', 'BMI_Y-2', ..., 'Î”SBP', 'Î”DBP', ...]
attention_scores = mean_attention.cpu().numpy()

plt.figure(figsize=(10, 6))
plt.barh(feature_names, attention_scores)
plt.xlabel('Average Attention Score')
plt.title('Feature Importance for High-Risk Patients')
plt.tight_layout()
plt.show()
```

#### Step 4: èˆ‡ SHAP æ¯”è¼ƒ

```python
import shap

# SHAP è§£é‡‹
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# æ¯”è¼ƒ
comparison_df = pd.DataFrame({
    'Feature': feature_names,
    'Attention_Score': attention_scores,
    'SHAP_Importance': abs(shap_values.values).mean(axis=0)
})

print(comparison_df.sort_values('Attention_Score', ascending=False))
```

---

## ğŸ“ˆ é æœŸç ”ç©¶æˆæœ

### è«–æ–‡å¯ä»¥å±•ç¤ºçš„å…§å®¹

#### 1. è­‰æ˜ç¥ç¶“ç¶²è·¯çš„å¯è§£é‡‹æ€§

**å°æ¯”**ï¼š
```
å‚³çµ±è§€é»ï¼š
  ç¥ç¶“ç¶²è·¯ = é»‘ç›’ï¼Œç„¡æ³•è§£é‡‹

æˆ‘å€‘çš„ç ”ç©¶ï¼š
  ç¥ç¶“ç¶²è·¯ + Attention = å¯è§£é‡‹
  - Attention Score é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§
  - èˆ‡ SHAP åˆ†æä¸€è‡´
  - èˆ‡é†«å­¸çŸ¥è­˜ä¸€è‡´
```

#### 2. ç™¼ç¾ç–¾ç—…ç‰¹å®šçš„é¢¨éšªå› å­

**é æœŸç™¼ç¾**ï¼š
```
é«˜è¡€å£“æ¨¡å‹çš„ Attentionï¼š
  - SBP è®ŠåŒ–ï¼ˆÎ”SBPï¼‰: 0.92 â­
  - Y-1 æ”¶ç¸®å£“: 0.78
  - å¹´é½¡: 0.65
  - BMI: 0.42

é«˜è¡€ç³–æ¨¡å‹çš„ Attentionï¼š
  - è¡€ç³–è®ŠåŒ–ï¼ˆÎ”Glucoseï¼‰: 0.88 â­
  - Y-1 ç©ºè…¹è¡€ç³–: 0.82
  - BMI: 0.58
  - å¹´é½¡: 0.61

â†’ ä¸åŒç–¾ç—…é—œæ³¨ä¸åŒçš„ç‰¹å¾µï¼
```

#### 3. å¤šä»»å‹™å­¸ç¿’çš„å…±äº«çŸ¥è­˜

**MTL çš„å„ªå‹¢**ï¼š
```
å–®ä»»å‹™å­¸ç¿’ (STL)ï¼š
  - é«˜è¡€å£“æ¨¡å‹åªå­¸é«˜è¡€å£“
  - é«˜è¡€ç³–æ¨¡å‹åªå­¸é«˜è¡€ç³–
  - é«˜è¡€è„‚æ¨¡å‹åªå­¸é«˜è¡€è„‚

å¤šä»»å‹™å­¸ç¿’ (MTL)ï¼š
  - å…±äº«å±¤å­¸ç¿’ã€Œä»£è¬å¥åº·ã€çš„é€šç”¨è¡¨ç¤º
  - ä¸‰å€‹ä»»å‹™äº’ç›¸å¹«åŠ©
  - Attention é¡¯ç¤ºå…±åŒé¢¨éšªå› å­ï¼ˆå¦‚ BMIã€å¹´é½¡ï¼‰
```

---

## ğŸ¯ ç¸½çµèˆ‡å»ºè­°

### æ ¸å¿ƒè¦é»

1. **Attention Score = æ¨¡å‹çš„ã€Œæ³¨æ„åŠ›ã€åˆ†å¸ƒ**
   - å‘Šè¨´ä½ æ¨¡å‹èªç‚ºå“ªäº›ç‰¹å¾µæœ€é‡è¦
   - æ•¸å€¼ç¯„åœ 0-1ï¼Œç¸½å’Œç‚º 1

2. **Taiwan MTL çš„æˆåŠŸç¶“é©—**
   - ç”¨ Attention ç™¼ç¾é«˜è¡€è„‚ã€å¤šé‡æ…¢æ€§ç—…ã€ç²¾ç¥å¥åº·ç­‰é¢¨éšªå› å­
   - çµæœèˆ‡é†«å­¸æ–‡ç»ä¸€è‡´ï¼Œé©—è­‰æ¨¡å‹å¯ä¿¡åº¦

3. **Attention vs SHAP**
   - Attentionï¼šç¥ç¶“ç¶²è·¯å…§å»ºï¼Œå³æ™‚è§£é‡‹
   - SHAPï¼šè·¨æ¨¡å‹æ¯”è¼ƒï¼Œç†è«–ä¿è­‰
   - **å»ºè­°å…©è€…éƒ½ç”¨**

### å°ä½ çš„ç ”ç©¶çš„å»ºè­°

#### âœ… ä¸»è¦ç­–ç•¥ï¼šSHAP + Attention é›™è»Œåˆ†æ

**SHAP åˆ†æ**ï¼ˆä¸»è¦ï¼‰ï¼š
- æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹ï¼ˆLR, DT, RF, SVM, NNï¼‰
- å›ç­”ï¼šInterpretable vs Black-box å“ªå€‹å¥½ï¼Ÿ
- å…¬å¹³æ¯”è¼ƒå¯è§£é‡‹æ€§

**Attention åˆ†æ**ï¼ˆåŠ åˆ†é …ï¼‰ï¼š
- å±•ç¤ºç¥ç¶“ç¶²è·¯çš„é¡å¤–å„ªå‹¢
- å³æ™‚è§£é‡‹èƒ½åŠ›
- ç™¼ç¾ç–¾ç—…ç‰¹å®šçš„é¢¨éšªæ¨¡å¼

#### âœ… å»ºè­°çš„æ¨¡å‹è¨­è¨ˆ

1. **Feature-level Attention** - å“ªäº›ç‰¹å¾µæœ€é‡è¦
2. **Task-specific Attention** - ä¸åŒç–¾ç—…é—œæ³¨ä¸åŒç‰¹å¾µ
3. **Temporal Attention** - Y-2 vs Y-1 å“ªå€‹æ™‚é–“é»æ›´é‡è¦

#### âœ… é æœŸè²¢ç»

1. **æ–¹æ³•è«–è²¢ç»**ï¼š
   - è­‰æ˜ MTL + Attention å¯æ‡‰ç”¨æ–¼ä¸‰é«˜é æ¸¬
   - ä¸éœ€è¦ ICD ä»£ç¢¼ï¼Œå•å·è³‡æ–™ä¹Ÿèƒ½ç”¨ Attention

2. **å¯¦å‹™è²¢ç»**ï¼š
   - è­˜åˆ¥å¯ä¿®æ”¹é¢¨éšªå› å­ï¼ˆå¦‚ BMIã€è¡€å£“è®ŠåŒ–ï¼‰
   - æä¾›å³æ™‚å¯è§£é‡‹çš„é æ¸¬çµæœ

3. **ç†è«–è²¢ç»**ï¼š
   - è­‰æ˜ç¥ç¶“ç¶²è·¯ä¸æ˜¯å®Œå…¨é»‘ç›’
   - Attention èˆ‡ SHAP çš„æ¯”è¼ƒåˆ†æ

---

## ğŸ“š å»¶ä¼¸é–±è®€

### ç›¸é—œæ–‡ç»

1. **Taiwan MTL (2025)**
   - DOI: 10.1038/s41598-025-99554-z
   - æª”æ¡ˆï¼š[Taiwan_MTL_2025_ä¸­æ–‡ç¿»è­¯.md](../references/Taiwan_MTL_2025_ä¸­æ–‡ç¿»è­¯.md)

2. **Attention æ©Ÿåˆ¶æ‡‰ç”¨æŒ‡å—**
   - æª”æ¡ˆï¼š[attention_mechanism_guide.md](../references/attention_mechanism_guide.md)

3. **SHAP ç›¸é—œ**
   - æª”æ¡ˆï¼š[AUC_memo.md](../literature_notes/AUC_memo.md)

### ç›¸é—œæ¦‚å¿µ

- [Multi-Task Learning](Model_Comparison_Plan.md)
- [Feature Selection](Feature_Selection_Marginal_Utility.md)
- [Model Interpretability](Class_Imbalance_in_Medical_Prediction.md)

---

**å»ºç«‹æ—¥æœŸ**: 2025å¹´
**æœ€å¾Œæ›´æ–°**: 2025å¹´
**ä½œè€…**: ç´€ä¼¯å–¬
**ç”¨é€”**: ç¢©å£«è«–æ–‡ç ”ç©¶å‚™å¿˜éŒ„
