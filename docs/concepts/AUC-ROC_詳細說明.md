# AUC-ROC 詳細說明

**適用場景**：二元分類問題，特別是資料不平衡的情況

**相關研究**：Liu et al. (2024) 台中榮總糖尿病預測研究

---

## 目錄

1. [基本概念](#基本概念)
2. [ROC 曲線](#roc-曲線)
3. [AUC 面積](#auc-面積)
4. [為何適合不平衡資料](#為何適合不平衡資料)
5. [論文實例](#論文實例)
6. [與 Accuracy 的比較](#與-accuracy-的比較)
7. [醫療應用考量](#醫療應用考量)
8. [口語化解釋](#口語化解釋)

---

## 基本概念

### ROC 是什麼？

**ROC = Receiver Operating Characteristic（接收者操作特徵曲線）**

這是一條曲線，用來視覺化分類模型在**不同閾值**下的表現。

### AUC 是什麼？

**AUC = Area Under the Curve（曲線下面積）**

就是 ROC 曲線下方的面積，範圍從 0 到 1。

---

## ROC 曲線

### 曲線的兩個軸

```
Y 軸（垂直）: True Positive Rate (TPR)
            = Sensitivity（敏感度）
            = Recall（召回率）
            = TP / (TP + FN)
            = 真正有病的人中，被正確預測為有病的比例

X 軸（水平）: False Positive Rate (FPR)
            = 1 - Specificity
            = FP / (FP + TN)
            = 真正沒病的人中，被錯誤預測為有病的比例
```

### 理想模型

- **TPR 高**：找到越多真正的病患越好
- **FPR 低**：誤判的正常人越少越好

---

### 什麼是「閾值」？

機器學習模型通常輸出一個**機率值**（0 到 1 之間），例如：

```
病患 A：模型預測糖尿病機率 = 0.85
病患 B：模型預測糖尿病機率 = 0.65
病患 C：模型預測糖尿病機率 = 0.30
```

我們需要設定一個**閾值（threshold）**來決定「多少機率以上算有病」：

#### 閾值 = 0.5

```
病患 A (0.85) → 預測為「有病」
病患 B (0.65) → 預測為「有病」
病患 C (0.30) → 預測為「正常」
```

#### 閾值 = 0.7

```
病患 A (0.85) → 預測為「有病」
病患 B (0.65) → 預測為「正常」（改變了！）
病患 C (0.30) → 預測為「正常」
```

### 閾值的影響

| 閾值設定 | TPR（敏感度）| FPR（假陽性率）| 特點 |
|---------|-------------|---------------|------|
| **低閾值（如 0.3）** | 高 | 高 | 容易預測為有病，不會漏掉病患，但誤判多 |
| **中閾值（如 0.5）** | 中 | 中 | 平衡 |
| **高閾值（如 0.7）** | 低 | 低 | 保守，只有很確定才預測有病，誤判少但會漏掉病患 |

---

### ROC 曲線怎麼畫？

ROC 曲線是將**所有可能的閾值**（從 0 到 1）都試一遍：
1. 每個閾值對應一個 (FPR, TPR) 點
2. 將所有點連起來就是 ROC 曲線

#### Liu et al. (2024) 論文的 ROC 曲線（Figure 2）

```
        TPR (Sensitivity)
        1.0 ┌─────────────────────────
            │         ╱XGBoost (AUC=0.93)
            │       ╱
        0.8 │     ╱  LogisticRegression (AUC=0.90)
            │   ╱
            │ ╱   RandomForest (AUC=0.74)
        0.6 │╱
            │
        0.4 │    對角線 = 隨機猜測 (AUC=0.5)
            │  ╱
        0.2 │╱
            │
        0.0 └─────────────────────────
           0.0  0.2  0.4  0.6  0.8  1.0
                FPR (1-Specificity)
```

**解讀**：
- 曲線越靠近左上角越好（高 TPR，低 FPR）
- XGBoost 的曲線最接近左上角
- 對角線代表隨機猜測（跟丟銅板一樣）

---

## AUC 面積

### AUC 的範圍與意義

| AUC 值 | 模型品質 | 說明 |
|--------|---------|------|
| **1.0** | 完美模型 | 所有病患都正確預測，所有正常人都正確預測 |
| **0.9 - 1.0** | 很好的模型 | 臨床上可用 |
| **0.8 - 0.9** | 好的模型 | 有實用價值 |
| **0.7 - 0.8** | 尚可的模型 | 需要改進 |
| **0.6 - 0.7** | 較差的模型 | 預測能力有限 |
| **0.5** | 隨機猜測 | 跟丟銅板一樣，沒有預測能力 |
| **< 0.5** | 比隨機還差 | 模型學反了 |

---

### 直觀理解 AUC

**AUC 可以理解為**：

> 隨機抽一個有病的人和一個沒病的人，模型給有病的人的分數比沒病的人高的機率。

#### 例子

**AUC = 0.93**（Liu et al. 的 XGBoost 模型）表示：

有 **93%** 的機會，模型給真正糖尿病患者的風險分數會高於正常人。

---

## 為何適合不平衡資料？

### Liu et al. (2024) 的資料分布

- **Class 0（正常）**：6,967 人（98.9%）
- **Class 1（糖尿病）**：76 人（1.1%）
- **比例**：約 92:1（極度不平衡）

---

### Accuracy 的問題

```python
# 懶惰模型：全部預測為正常
for patient in all_patients:
    predict(patient) = "正常"

# 結果
Accuracy = 6967 / 7043 = 98.9% ✅ 看起來很好！

# 但實際上
TPR (Recall) = 0 / 76 = 0% ❌ 完全沒找到任何糖尿病患者
```

**問題核心**：
- Accuracy 被多數類別（Class 0）主導
- 模型可以忽略少數類別（Class 1），仍然得到高 Accuracy
- 這在醫療應用中是災難性的（漏掉所有病患）

---

### AUC-ROC 的優勢

AUC-ROC 會考慮：

1. **TPR（敏感度）**
   - 有沒有找到真正的病患？
   - 對應 Y 軸

2. **FPR（假陽性率）**
   - 有沒有誤判太多正常人？
   - 對應 X 軸

3. **所有閾值下的表現**
   - 不是只看單一閾值
   - 綜合評估模型的分辨能力

**關鍵優勢**：

即使資料極度不平衡，AUC-ROC 仍然能**真實反映模型的分辨能力**，不會被多數類別誤導。

---

## 論文實例

### Liu et al. (2024) 的三個模型比較

| 模型 | Accuracy | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) | **AUC-ROC** |
|------|----------|---------------------|------------------|-------------------|-------------|
| Random Forest | **99%** | 25% | 1% | 0.03 | **0.74** |
| Logistic Regression | **99%** | 0% | 0% | 0.00 | **0.90** |
| XGBoost | 98% | 19% | 33% | 0.24 | **0.93** ⭐ |

---

### 詳細分析

#### 模型 1: Random Forest

```
Accuracy: 99%（看起來很好）
Recall (Class 1): 1%（幾乎沒找到糖尿病患者）
AUC-ROC: 0.74（其實不好）
```

**問題**：
- 模型幾乎全預測為正常
- Accuracy 高是因為資料不平衡
- **AUC 揭露了真相：預測能力一般**

**Confusion Matrix**：
```
                預測 0    預測 1
實際 0 (正常)    3593       1
實際 1 (糖尿病)   60        0
```

→ 60 個糖尿病患者中，**找到 0 個**

---

#### 模型 2: Logistic Regression

```
Accuracy: 99%（看起來很好）
Recall (Class 1): 0%（完全沒找到糖尿病患者）
AUC-ROC: 0.90（還可以，但不是最好）
```

**問題**：
- 雖然 AUC 不錯（0.90）
- 但實際應用時 Recall 太低（0%）
- 需要調整閾值才能發揮作用

**Confusion Matrix**：
```
                預測 0    預測 1
實際 0 (正常)    3031      563
實際 1 (糖尿病)   14        46
```

→ Recall = 46 / (14+46) = 76.7%（如果調整閾值可以改善）

---

#### 模型 3: XGBoost ⭐

```
Accuracy: 98%（最低）
Recall (Class 1): 33%（找到 1/3 的糖尿病患者）
AUC-ROC: 0.93（最高）✅
```

**優勢**：
- 雖然 Accuracy 最低（98% vs 99%）
- 但 **AUC 最高**（0.93）
- **Recall 最好**（33%），真正有預測能力
- 在醫療應用中最實用

**Confusion Matrix**：
```
                預測 0    預測 1
實際 0 (正常)    3703      252
實際 1 (糖尿病)   33        32
```

→ 找到 32 / 65 = 49.2% 的糖尿病患者

---

### 結論

| 模型 | Accuracy 排名 | AUC-ROC 排名 | 實際預測能力 |
|------|--------------|-------------|-------------|
| Random Forest | 1（最高 99%）| 3（最低 0.74）| ❌ 最差 |
| Logistic Regression | 1（最高 99%）| 2（中等 0.90）| ⚠️ 中等 |
| **XGBoost** | 3（最低 98%）| **1（最高 0.93）** | ✅ **最好** |

**關鍵洞察**：
- **Accuracy 會騙人**（被多數類別主導）
- **AUC-ROC 才是真正的分辨能力指標**
- XGBoost 是最佳模型

---

## 與 Accuracy 的比較

### Accuracy 的定義

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
         = 預測正確的總數 / 總樣本數
```

### 為何 Accuracy 不可靠？

#### 情境：極度不平衡資料

```
總樣本：7,043 人
- 正常：6,967 人（98.9%）
- 糖尿病：76 人（1.1%）
```

#### 懶惰模型

```python
# 模型策略：全部預測為正常
def lazy_model(patient):
    return "正常"

# 結果
TP = 0（正確預測的糖尿病患者）
TN = 6967（正確預測的正常人）
FP = 0（誤判為糖尿病的正常人）
FN = 76（漏掉的糖尿病患者）

Accuracy = (0 + 6967) / 7043 = 98.9% ✅

但：
Recall = 0 / (0 + 76) = 0% ❌
Precision = 無法計算（分母為 0）
```

**問題**：
- Accuracy 高達 98.9%，看起來很好
- 但**完全沒有預測到任何糖尿病患者**
- 在醫療應用中毫無用處

---

### AUC-ROC vs Accuracy

| 特性 | Accuracy | AUC-ROC |
|------|----------|---------|
| **受資料不平衡影響** | ✅ 是（嚴重）| ❌ 否 |
| **考慮所有閾值** | ❌ 否（只看單一閾值）| ✅ 是 |
| **反映分辨能力** | ❌ 否（會被多數類別主導）| ✅ 是 |
| **適合醫療應用** | ❌ 否 | ✅ 是 |
| **易於理解** | ✅ 是 | ⚠️ 需要解釋 |

---

### 什麼時候可以用 Accuracy？

Accuracy 適用於：
- 資料**平衡**的情況（各類別樣本數接近）
- 各類別的重要性相同
- 不需要調整閾值

Accuracy **不適用**於：
- 資料**極度不平衡**（如 92:1）
- 少數類別特別重要（如醫療診斷）
- 需要權衡敏感度和特異度

---

## 醫療應用考量

### 為何醫療預測重視 AUC-ROC？

#### 1. 不能漏掉病患（高 Recall/Sensitivity）

```
寧可：
- 多檢查 10 個正常人（False Positive）

也不要：
- 漏掉 1 個真正的病患（False Negative）
```

**原因**：
- 漏掉病患可能延誤治療
- 多檢查只是多花一點錢，但不會造成嚴重後果

---

#### 2. 控制誤判率（低 FPR/高 Specificity）

```
不希望：
- 太多正常人被誤判為有病（造成恐慌）
- 浪費醫療資源
```

**原因**：
- 誤判會造成心理壓力
- 後續檢查浪費資源
- 降低篩檢計畫的可行性

---

#### 3. 彈性調整閾值

醫療決策需要根據情境調整：

**情境 A：大規模篩檢**
- 目標：找出所有可能的病患
- 策略：**降低閾值**（機率 > 0.3 就轉介）
- 結果：高 Recall，FPR 稍高可接受

**情境 B：確診前的最後評估**
- 目標：減少誤診
- 策略：**提高閾值**（機率 > 0.7 才確診）
- 結果：高 Precision，Recall 稍低可接受

**AUC-ROC 的優勢**：
- 顯示模型在**所有閾值**下的表現
- 讓臨床醫生可以根據需求選擇最佳閾值
- Accuracy 無法提供這種彈性

---

### 醫療預測的評估指標優先順序

1. **AUC-ROC** - 最重要，評估整體分辨能力
2. **Recall/Sensitivity** - 不能漏掉病患
3. **Specificity** - 控制誤判率
4. **F1-Score** - 平衡 Precision 和 Recall
5. **Accuracy** - 參考用，不可當作主要指標

---

## 口語化解釋

### 版本 1：簡短版（30 秒）

> 在資料極度不平衡的情況下，準確率會騙人。因為如果 98.9% 的人都是正常的，模型只要全部預測為正常，準確率就有 98.9%，但這完全沒有預測能力。
>
> AUC-ROC 是更嚴謹的指標，它會看模型在所有可能的判斷標準下，能不能區分有病和沒病的人。AUC 越接近 1 表示區分能力越好，0.5 就跟隨機猜測一樣。
>
> 在這個研究中，XGBoost 的 AUC 達到 0.93，表示有 93% 的機會，模型給糖尿病患者的風險分數會高於正常人，這才是真正有用的預測模型。

---

### 版本 2：詳細版（1 分鐘）

> 讓我解釋為什麼這個研究用 AUC-ROC 而不是準確率來評估模型。
>
> 這個研究的資料極度不平衡，6967 個正常人，只有 76 個糖尿病患者，比例是 92 比 1。在這種情況下，如果模型很懶，全部預測為正常，準確率就有 98.9%，看起來很好，但實際上完全沒有找到任何糖尿病患者，這在醫療應用中是災難性的。
>
> AUC-ROC 是什麼？ROC 是一條曲線，顯示模型在所有可能的判斷標準下，找到病患的能力（敏感度）和誤判正常人的比例（假陽性率）。AUC 就是這條曲線下方的面積，範圍從 0 到 1。
>
> AUC 接近 1 表示模型很會區分有病和沒病的人，0.5 就跟隨機猜測一樣。在這個研究中，Random Forest 雖然準確率 99%，但 AUC 只有 0.74，顯示它預測能力一般。XGBoost 的準確率雖然最低，只有 98%，但 AUC 達到 0.93，是三個模型中最好的，而且它找到了 33% 的糖尿病患者，這才是真正有用的預測模型。
>
> 所以結論是，在資料不平衡的情況下，準確率會被多數類別主導，AUC-ROC 才是真正反映模型分辨能力的指標。

---

### 版本 3：比喻版（給非技術背景者）

> 想像一個情境：你要從 100 個人中找出 1 個小偷。
>
> **懶惰的警察**（高 Accuracy，低 AUC）：
> - 策略：「大家都是好人」
> - 結果：99% 準確（99 個好人都說對了）
> - 問題：完全沒抓到小偷
>
> **認真的警察**（略低 Accuracy，高 AUC）：
> - 策略：根據線索（可疑程度）判斷
> - 結果：97% 準確（抓到小偷，但誤判了 2 個好人）
> - 優點：真正有用
>
> 在醫療診斷中，我們需要「認真的警察」，能夠真正找出病患的模型。AUC-ROC 就是評估這種「真正的能力」的指標，而不是只看表面的準確率。

---

## 總結

### 核心要點

1. **ROC 曲線**顯示模型在所有閾值下的 TPR vs FPR
2. **AUC** 是 ROC 曲線下面積，範圍 0-1
3. **AUC = 0.93** 表示模型有 93% 的機會正確區分病患和正常人
4. **不受資料不平衡影響**，真實反映分辨能力
5. **Accuracy 會被多數類別主導**，在不平衡資料中不可靠

---

### 實務建議

#### 當資料不平衡時

1. ✅ **使用 AUC-ROC 作為主要評估指標**
2. ✅ 同時檢查 Recall（不能漏掉少數類別）
3. ✅ 使用 Confusion Matrix 了解模型行為
4. ❌ 不要只看 Accuracy

#### 當資料平衡時

1. ✅ Accuracy 可以作為主要指標
2. ✅ 但仍建議檢查 AUC-ROC
3. ✅ 綜合多個指標評估

---

## 相關資源

### 論文

- Liu et al. (2024). Use of Machine Learning to Predict the Incidence of Type 2 Diabetes Among Relatively Healthy Adults: A 10-Year Longitudinal Study in Taiwan. Diagnostics, 15(1), 72.
  - 檔案位置：`docs/references/diagnostics-15-00072.pdf`
  - 深度解析：`docs/literature_notes/Liu_2024_TCVGH_Diabetes_Prediction_深度解析.md`

### 延伸閱讀

- 混淆矩陣（Confusion Matrix）
- Precision, Recall, F1-Score
- 資料不平衡處理方法（SMOTE, 權重調整等）

---

## 筆記整理日期

2025年10月7日

---

## 關鍵標籤

`#AUC-ROC` `#評估指標` `#資料不平衡` `#醫療預測` `#模型評估` `#機器學習`
